{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import csv\r\n",
    "import easydict\r\n",
    "import cv2\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import transforms\r\n",
    "from torchvision.utils import save_image\r\n",
    "from torch.autograd import Variable\r\n",
    "\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "from time import sleep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from torchvision import utils\r\n",
    "from torchvision import datasets\r\n",
    "import itertools\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import random\r\n",
    "random.seed(100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "mnist_train = datasets.MNIST(root='./MNIST_data/', # 다운로드 경로 지정\r\n",
    "                          train=True, # True를 지정하면 훈련 데이터로 다운로드\r\n",
    "                          transform=transforms.ToTensor(), # 텐서로 변환\r\n",
    "                          download=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "9913344it [00:00, 20062832.52it/s]                            \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "29696it [00:00, 1488598.95it/s]          \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1649664it [00:00, 1700855.05it/s]                             \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "5120it [00:00, 5136291.91it/s]          "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ./MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "C:\\Users\\akzns\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n",
    "                                          batch_size=1,\r\n",
    "                                          shuffle=True,\r\n",
    "                                          drop_last=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "total_batch = len(train_dataloader)\r\n",
    "print(total_batch)\r\n",
    "a,b=next(iter(train_dataloader))\r\n",
    "print(a.shape, b.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n",
      "torch.Size([1, 1, 28, 28]) torch.Size([1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data = list()\r\n",
    "train_label = list()\r\n",
    "\r\n",
    "for i, data in enumerate(train_dataloader):\r\n",
    "    imgs, labels = data\r\n",
    "    train_data.append(np.asarray(imgs.squeeze().view(28,28,1)))\r\n",
    "    train_label.append(int(labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "num_img_per_class = 3000\r\n",
    "class_list=datasets.MNIST.classes\r\n",
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4588, 0.8314, 1.0000,\n",
      "           0.7216, 0.5020, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0745, 0.3804, 0.9412, 0.9922, 0.9961, 0.9961,\n",
      "           0.9961, 0.9961, 0.7804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.3569, 0.9451, 0.9961, 0.7412, 0.4157, 0.0824, 0.0314,\n",
      "           0.0314, 0.3451, 0.1647, 0.0275, 0.2902, 0.0784, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.3529, 0.9922, 0.9294, 0.3451, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.6118, 0.9961, 0.7059, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.9255, 0.9294, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0627, 0.8980, 0.9961, 0.4078, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.9490, 0.7412, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.2314, 0.7333, 0.9961, 0.9804, 0.2078, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.9490, 0.9686, 0.3922, 0.2235, 0.2235, 0.2863, 0.5569, 0.5569,\n",
      "           0.8353, 0.9765, 0.9961, 0.9961, 0.4353, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.5216, 0.9804, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
      "           0.9961, 0.9255, 0.9961, 0.9765, 0.2000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.2118, 0.4745, 0.7765, 0.6235, 0.7647, 0.5216, 0.3059,\n",
      "           0.1333, 0.7765, 0.9961, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n",
      "           0.4941, 0.9647, 0.9765, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529,\n",
      "           0.9961, 0.9961, 0.4471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176,\n",
      "           0.9961, 0.9804, 0.1882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7608,\n",
      "           0.9961, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686, 0.9569,\n",
      "           0.9333, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1098, 0.8275, 0.9961,\n",
      "           0.6863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7765, 0.9961, 0.9333,\n",
      "           0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.9137, 0.9961, 0.5765,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0078, 0.7882, 0.9961, 0.9216, 0.1059,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.4510, 0.9961, 0.9922, 0.2510, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0706, 0.9176, 0.6941, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "           0.0000, 0.0000, 0.0000, 0.0000]]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "a.size()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class My_MNIST_Dataset(Dataset):\r\n",
    "\r\n",
    "    def __init__(self, data, labels, transform=None):\r\n",
    "        \"\"\"\r\n",
    "        Args:\r\n",
    "            csv_file (string): csv file path\r\n",
    "            root_dir (string): Directory path where all images exist\r\n",
    "            transform (callable, optional): Optical transform to be applied to the sample\r\n",
    "        \"\"\"\r\n",
    "        self.data = data\r\n",
    "        self.labels = labels\r\n",
    "        # MNIST dataset class count\r\n",
    "        self.classnum = 10\r\n",
    "        self.imsize = parser['img_size']\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        # data resize and one-hot encoding\r\n",
    "        data_torch = torch.from_numpy(cv2.resize(self.data[idx],(self.imsize,self.imsize)))\r\n",
    "        data_torch2vec = data_torch.view(1*self.imsize*self.imsize)\r\n",
    "\r\n",
    "        # core\r\n",
    "        label_torch = torch.from_numpy(np.zeros(self.classnum))\r\n",
    "        label_torch[self.labels[idx]] = 1\r\n",
    "        return data_torch2vec, label_torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "parser = easydict.EasyDict({\"n_epochs\": 200,\r\n",
    "                            \"batch_size\":100, \r\n",
    "                            \"lr\":0.0002, \r\n",
    "                            \"b1\":0.5, \r\n",
    "                            \"b2\":0.999, \r\n",
    "                            \"n_cpu\":8, \r\n",
    "                            \"latent_dim\":100, \r\n",
    "                            \"n_classes\":len(class_list), \r\n",
    "                            \"img_size\":28, \r\n",
    "                            \"channels\":3, \r\n",
    "                            \"sample_interval\":1})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "cuda = True if torch.cuda.is_available() else False\r\n",
    "img_shape = (parser.channels, parser.img_size, parser.img_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "img_shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generator part"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# model class\r\n",
    "class Generator(torch.nn.Module):\r\n",
    "  def __init__(self):\r\n",
    "    super(Generator, self).__init__()\r\n",
    "    size = parser['img_size']\r\n",
    "\r\n",
    "    # z = noise, y = class for consistency\r\n",
    "    self.layer_z = torch.nn.Linear(100, 200)\r\n",
    "    self.relu_z = torch.nn.ReLU()\r\n",
    "\r\n",
    "    self.layer_y = torch.nn.Linear(parser.n_classes, 1000)\r\n",
    "    self.relu_y = torch.nn.ReLU()\r\n",
    "\r\n",
    "    self.layer_out = torch.nn.Linear(1200, 1*size*size)\r\n",
    "\r\n",
    "  def forward(self, z, y):\r\n",
    "    z_hidden = self.relu_z(self.layer_z(z))\r\n",
    "    y_hidden = self.relu_y(self.layer_y(y))\r\n",
    "    \r\n",
    "\r\n",
    "    concat_hidden = torch.cat([z_hidden,y_hidden],dim=1)\r\n",
    "    output_vec = self.layer_out(concat_hidden)\r\n",
    "    output = torch.sigmoid(output_vec)\r\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discriminator part"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# model class\r\n",
    "class Discriminator(torch.nn.Module):\r\n",
    "  def __init__(self):\r\n",
    "    super(Discriminator, self).__init__()\r\n",
    "    size = parser['img_size']\r\n",
    "\r\n",
    "    self.layer_x = torch.nn.Linear(1*size*size, 1200)\r\n",
    "    self.relu_x = torch.nn.ReLU()\r\n",
    "\r\n",
    "    self.layer_y = torch.nn.Linear(parser.n_classes, 250)\r\n",
    "    self.relu_y = torch.nn.ReLU()\r\n",
    "\r\n",
    "    self.layer_out = torch.nn.Linear(1450, 960)\r\n",
    "    self.relu_out = torch.nn.ReLU()\r\n",
    "\r\n",
    "    self.layer_score = torch.nn.Linear(960, 1)\r\n",
    "\r\n",
    "  def forward(self, x, y):\r\n",
    "    x_hidden = self.relu_x(self.layer_x(x))\r\n",
    "    y_hidden = self.relu_y(self.layer_y(y))\r\n",
    "    concat_hidden = torch.cat([x_hidden, y_hidden], dim=1)\r\n",
    "# TO DO (4) starts here\r\n",
    "    output_vec = self.relu_out(self.layer_out(concat_hidden))#Fill Here#\r\n",
    "    output_score = self.layer_score(output_vec)#Fill Here#\r\n",
    "    output = torch.sigmoid(output_score)#Fill Here#\r\n",
    "# TO DO (4) ends here\r\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sample image checking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def sample_image(z, n_row, epoch):\r\n",
    "    size = parser['img_size']\r\n",
    "    # Sample noise\r\n",
    "    gen_labels = []\r\n",
    "    # Get labels ranging from 0 to n_classes for n rows\r\n",
    "    for randpos in range(10):\r\n",
    "      gen_labels.append(torch.eye(parser.n_classes)[randpos])\r\n",
    "    gen_labels = torch.stack(gen_labels).cuda()\r\n",
    "    gen_imgs = generator(z, gen_labels)\r\n",
    "    save_image(gen_imgs.view(n_row,1,size,size).data, \"./assignment4_arrange_images/assignment4_arrange_images4/%s.png\" % str(epoch).zfill(3), nrow=n_row, normalize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Main\r\n",
    "adversarial_loss = torch.nn.MSELoss().cuda()\r\n",
    "generator = Generator().cuda()\r\n",
    "discriminator = Discriminator().cuda()\r\n",
    "\r\n",
    "mnist_train_dataset = My_MNIST_Dataset(train_data, train_label, parser['img_size'])                   \r\n",
    "\r\n",
    "mnist_train_dataloader = DataLoader(mnist_train_dataset, batch_size=parser.batch_size, shuffle=True)\r\n",
    "\r\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002)\r\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Directory 생성\r\n",
    "if not os.path.exists('./assignment4_arrange_images/assignment4_arrange_images4'):\r\n",
    "  os.makedirs('./assignment4_arrange_images/assignment4_arrange_images4')\r\n",
    "\r\n",
    "z_test = torch.randn(10, parser.latent_dim).type(torch.FloatTensor).cuda()\r\n",
    "\r\n",
    "# Train\r\n",
    "generator.train()\r\n",
    "discriminator.train()\r\n",
    "\r\n",
    "g_loss = torch.Tensor([0])\r\n",
    "d_loss = torch.Tensor([0])\r\n",
    "\r\n",
    "for epoch in range(parser.n_epochs):\r\n",
    "  for batch_idx, (img_torch2vec, label_torch) in enumerate(mnist_train_dataloader):\r\n",
    "    img_torch2vec = img_torch2vec.type(torch.FloatTensor).cuda()  #to GPU\r\n",
    "    label_torch = label_torch.type(torch.FloatTensor).cuda()\r\n",
    "\r\n",
    "    # Adversarial ground truths\r\n",
    "    valid = torch.ones(parser.batch_size, 1).cuda()\r\n",
    "    fake = torch.zeros(parser.batch_size, 1).cuda()\r\n",
    "\r\n",
    "    # Configure input\r\n",
    "    real_imgs = img_torch2vec\r\n",
    "    labels = label_torch\r\n",
    "\r\n",
    "    # Train Gen\r\n",
    "    optimizer_G.zero_grad()\r\n",
    "\r\n",
    "    # Sample noise and labels as generator input\r\n",
    "    z = torch.randn(parser.batch_size, parser.latent_dim).cuda()\r\n",
    "    gen_labels = []\r\n",
    "    for randpos in np.random.randint(0, parser.n_classes, parser.batch_size):\r\n",
    "      gen_labels.append(torch.eye(parser.n_classes)[randpos])\r\n",
    "    gen_labels = torch.stack(gen_labels).cuda()\r\n",
    "\r\n",
    "    # Generate a batch of images\r\n",
    "    gen_imgs = generator(z, gen_labels)\r\n",
    "    \r\n",
    "    # Loss measures generator's ability to fool the discriminator\r\n",
    "    val_output = discriminator(gen_imgs, gen_labels)\r\n",
    "    # g_loss = adversarial_loss(val_output, valid)\r\n",
    "    g_loss = -torch.mean(torch.log(val_output + 1e-8))\r\n",
    "\r\n",
    "    g_loss.backward()\r\n",
    "    optimizer_G.step()\r\n",
    "\r\n",
    "    if batch_idx % 4 == 0:\r\n",
    "      # Train Disc\r\n",
    "      optimizer_D.zero_grad()\r\n",
    "      \r\n",
    "      validity_real = discriminator(real_imgs, labels)\r\n",
    "      d_real_loss = adversarial_loss(validity_real, valid)\r\n",
    "\r\n",
    "      # val = output         \r\n",
    "      validity_fake = discriminator(gen_imgs.detach(), gen_labels)\r\n",
    "      d_fake_loss = adversarial_loss(validity_fake, fake)\r\n",
    "\r\n",
    "      # d_loss = (d_real_loss + d_fake_loss) / 2\r\n",
    "      d_loss = -torch.mean(torch.log(validity_real + 1e-8) + torch.log(1 - validity_fake + 1e-8))\r\n",
    "\r\n",
    "      d_loss.backward()\r\n",
    "      optimizer_D.step()\r\n",
    "    if batch_idx % 100 == 0:\r\n",
    "      print('{:<13s}{:<8s}{:<6s}{:<10s}{:<8s}{:<9.5f}{:<8s}{:<9.5f}'.format('Train Epoch: ', '[' + str(epoch) + '/' + str(parser['n_epochs']) + ']', 'Step: ', '[' + str(batch_idx) + '/' + str(len(mnist_train_dataloader)) + ']', 'G loss: ', g_loss.item(), 'D loss: ', d_loss.item()))\r\n",
    "\r\n",
    "  if epoch % parser.sample_interval == 0:\r\n",
    "    sample_image(z_test, n_row=10, epoch=epoch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: [0/200] Step: [0/600]   G loss: 0.68158  D loss: 1.38345  \n",
      "Train Epoch: [0/200] Step: [100/600] G loss: 2.01697  D loss: 0.38278  \n",
      "Train Epoch: [0/200] Step: [200/600] G loss: 1.56921  D loss: 0.41538  \n",
      "Train Epoch: [0/200] Step: [300/600] G loss: 1.53077  D loss: 0.58407  \n",
      "Train Epoch: [0/200] Step: [400/600] G loss: 2.54342  D loss: 0.67567  \n",
      "Train Epoch: [0/200] Step: [500/600] G loss: 2.95479  D loss: 0.29972  \n",
      "Train Epoch: [1/200] Step: [0/600]   G loss: 2.55594  D loss: 0.32471  \n",
      "Train Epoch: [1/200] Step: [100/600] G loss: 3.45636  D loss: 0.10174  \n",
      "Train Epoch: [1/200] Step: [200/600] G loss: 4.70273  D loss: 0.08725  \n",
      "Train Epoch: [1/200] Step: [300/600] G loss: 3.11912  D loss: 0.34770  \n",
      "Train Epoch: [1/200] Step: [400/600] G loss: 3.68498  D loss: 0.24417  \n",
      "Train Epoch: [1/200] Step: [500/600] G loss: 3.88842  D loss: 0.08626  \n",
      "Train Epoch: [2/200] Step: [0/600]   G loss: 4.01800  D loss: 0.04227  \n",
      "Train Epoch: [2/200] Step: [100/600] G loss: 4.04896  D loss: 0.12749  \n",
      "Train Epoch: [2/200] Step: [200/600] G loss: 3.08926  D loss: 0.16645  \n",
      "Train Epoch: [2/200] Step: [300/600] G loss: 3.10624  D loss: 0.13494  \n",
      "Train Epoch: [2/200] Step: [400/600] G loss: 3.21188  D loss: 0.14382  \n",
      "Train Epoch: [2/200] Step: [500/600] G loss: 2.39747  D loss: 0.37333  \n",
      "Train Epoch: [3/200] Step: [0/600]   G loss: 2.16276  D loss: 0.49150  \n",
      "Train Epoch: [3/200] Step: [100/600] G loss: 2.50479  D loss: 0.66870  \n",
      "Train Epoch: [3/200] Step: [200/600] G loss: 3.47605  D loss: 0.18079  \n",
      "Train Epoch: [3/200] Step: [300/600] G loss: 3.21781  D loss: 0.08642  \n",
      "Train Epoch: [3/200] Step: [400/600] G loss: 3.26523  D loss: 0.10512  \n",
      "Train Epoch: [3/200] Step: [500/600] G loss: 4.20452  D loss: 0.24185  \n",
      "Train Epoch: [4/200] Step: [0/600]   G loss: 4.52986  D loss: 0.01890  \n",
      "Train Epoch: [4/200] Step: [100/600] G loss: 5.65915  D loss: 0.04536  \n",
      "Train Epoch: [4/200] Step: [200/600] G loss: 4.83408  D loss: 0.04464  \n",
      "Train Epoch: [4/200] Step: [300/600] G loss: 3.85307  D loss: 0.07830  \n",
      "Train Epoch: [4/200] Step: [400/600] G loss: 4.05072  D loss: 0.04418  \n",
      "Train Epoch: [4/200] Step: [500/600] G loss: 3.11701  D loss: 0.08431  \n",
      "Train Epoch: [5/200] Step: [0/600]   G loss: 3.51939  D loss: 0.20256  \n",
      "Train Epoch: [5/200] Step: [100/600] G loss: 2.49230  D loss: 0.23008  \n",
      "Train Epoch: [5/200] Step: [200/600] G loss: 4.14997  D loss: 0.16064  \n",
      "Train Epoch: [5/200] Step: [300/600] G loss: 3.99939  D loss: 0.20800  \n",
      "Train Epoch: [5/200] Step: [400/600] G loss: 2.89547  D loss: 0.16119  \n",
      "Train Epoch: [5/200] Step: [500/600] G loss: 4.61177  D loss: 0.02923  \n",
      "Train Epoch: [6/200] Step: [0/600]   G loss: 3.89435  D loss: 0.04271  \n",
      "Train Epoch: [6/200] Step: [100/600] G loss: 4.41268  D loss: 0.02564  \n",
      "Train Epoch: [6/200] Step: [200/600] G loss: 5.45443  D loss: 0.01167  \n",
      "Train Epoch: [6/200] Step: [300/600] G loss: 3.68912  D loss: 0.06711  \n",
      "Train Epoch: [6/200] Step: [400/600] G loss: 5.27307  D loss: 0.12744  \n",
      "Train Epoch: [6/200] Step: [500/600] G loss: 6.06732  D loss: 0.06321  \n",
      "Train Epoch: [7/200] Step: [0/600]   G loss: 4.40450  D loss: 0.11737  \n",
      "Train Epoch: [7/200] Step: [100/600] G loss: 3.49905  D loss: 0.17650  \n",
      "Train Epoch: [7/200] Step: [200/600] G loss: 2.92368  D loss: 0.13977  \n",
      "Train Epoch: [7/200] Step: [300/600] G loss: 4.68525  D loss: 0.23655  \n",
      "Train Epoch: [7/200] Step: [400/600] G loss: 4.63537  D loss: 0.02878  \n",
      "Train Epoch: [7/200] Step: [500/600] G loss: 4.52243  D loss: 0.13297  \n",
      "Train Epoch: [8/200] Step: [0/600]   G loss: 3.60875  D loss: 0.10802  \n",
      "Train Epoch: [8/200] Step: [100/600] G loss: 4.89745  D loss: 0.15685  \n",
      "Train Epoch: [8/200] Step: [200/600] G loss: 4.25249  D loss: 0.06812  \n",
      "Train Epoch: [8/200] Step: [300/600] G loss: 4.82121  D loss: 0.04449  \n",
      "Train Epoch: [8/200] Step: [400/600] G loss: 4.28660  D loss: 0.24062  \n",
      "Train Epoch: [8/200] Step: [500/600] G loss: 4.06611  D loss: 0.09784  \n",
      "Train Epoch: [9/200] Step: [0/600]   G loss: 4.44555  D loss: 0.14798  \n",
      "Train Epoch: [9/200] Step: [100/600] G loss: 4.05212  D loss: 0.11000  \n",
      "Train Epoch: [9/200] Step: [200/600] G loss: 3.70002  D loss: 0.07078  \n",
      "Train Epoch: [9/200] Step: [300/600] G loss: 3.70856  D loss: 0.11208  \n",
      "Train Epoch: [9/200] Step: [400/600] G loss: 4.29262  D loss: 0.03495  \n",
      "Train Epoch: [9/200] Step: [500/600] G loss: 3.51670  D loss: 0.10635  \n",
      "Train Epoch: [10/200]Step: [0/600]   G loss: 3.67536  D loss: 0.09431  \n",
      "Train Epoch: [10/200]Step: [100/600] G loss: 3.84584  D loss: 0.15493  \n",
      "Train Epoch: [10/200]Step: [200/600] G loss: 5.17452  D loss: 0.15796  \n",
      "Train Epoch: [10/200]Step: [300/600] G loss: 3.69605  D loss: 0.30619  \n",
      "Train Epoch: [10/200]Step: [400/600] G loss: 4.01108  D loss: 0.10119  \n",
      "Train Epoch: [10/200]Step: [500/600] G loss: 3.66450  D loss: 0.11171  \n",
      "Train Epoch: [11/200]Step: [0/600]   G loss: 3.91059  D loss: 0.10344  \n",
      "Train Epoch: [11/200]Step: [100/600] G loss: 4.24447  D loss: 0.09753  \n",
      "Train Epoch: [11/200]Step: [200/600] G loss: 3.63342  D loss: 0.15538  \n",
      "Train Epoch: [11/200]Step: [300/600] G loss: 3.52782  D loss: 0.28950  \n",
      "Train Epoch: [11/200]Step: [400/600] G loss: 3.76375  D loss: 0.36437  \n",
      "Train Epoch: [11/200]Step: [500/600] G loss: 3.97205  D loss: 0.17728  \n",
      "Train Epoch: [12/200]Step: [0/600]   G loss: 7.24339  D loss: 0.08276  \n",
      "Train Epoch: [12/200]Step: [100/600] G loss: 4.49683  D loss: 0.18884  \n",
      "Train Epoch: [12/200]Step: [200/600] G loss: 3.70142  D loss: 0.21608  \n",
      "Train Epoch: [12/200]Step: [300/600] G loss: 4.21970  D loss: 0.12012  \n",
      "Train Epoch: [12/200]Step: [400/600] G loss: 2.80402  D loss: 0.23572  \n",
      "Train Epoch: [12/200]Step: [500/600] G loss: 5.48978  D loss: 0.16180  \n",
      "Train Epoch: [13/200]Step: [0/600]   G loss: 4.26146  D loss: 0.14533  \n",
      "Train Epoch: [13/200]Step: [100/600] G loss: 3.70270  D loss: 0.18793  \n",
      "Train Epoch: [13/200]Step: [200/600] G loss: 3.03916  D loss: 0.34207  \n",
      "Train Epoch: [13/200]Step: [300/600] G loss: 2.88471  D loss: 0.16792  \n",
      "Train Epoch: [13/200]Step: [400/600] G loss: 4.19455  D loss: 0.24365  \n",
      "Train Epoch: [13/200]Step: [500/600] G loss: 4.23234  D loss: 0.20066  \n",
      "Train Epoch: [14/200]Step: [0/600]   G loss: 4.26476  D loss: 0.16952  \n",
      "Train Epoch: [14/200]Step: [100/600] G loss: 3.44893  D loss: 0.26190  \n",
      "Train Epoch: [14/200]Step: [200/600] G loss: 3.91962  D loss: 0.13960  \n",
      "Train Epoch: [14/200]Step: [300/600] G loss: 4.35074  D loss: 0.20784  \n",
      "Train Epoch: [14/200]Step: [400/600] G loss: 5.06768  D loss: 0.11818  \n",
      "Train Epoch: [14/200]Step: [500/600] G loss: 3.91851  D loss: 0.12245  \n",
      "Train Epoch: [15/200]Step: [0/600]   G loss: 4.47273  D loss: 0.21327  \n",
      "Train Epoch: [15/200]Step: [100/600] G loss: 3.33711  D loss: 0.42472  \n",
      "Train Epoch: [15/200]Step: [200/600] G loss: 3.22637  D loss: 0.18888  \n",
      "Train Epoch: [15/200]Step: [300/600] G loss: 4.81673  D loss: 0.10920  \n",
      "Train Epoch: [15/200]Step: [400/600] G loss: 4.29145  D loss: 0.22128  \n",
      "Train Epoch: [15/200]Step: [500/600] G loss: 4.53070  D loss: 0.05779  \n",
      "Train Epoch: [16/200]Step: [0/600]   G loss: 3.37410  D loss: 0.24177  \n",
      "Train Epoch: [16/200]Step: [100/600] G loss: 4.54131  D loss: 0.32985  \n",
      "Train Epoch: [16/200]Step: [200/600] G loss: 4.69046  D loss: 0.20544  \n",
      "Train Epoch: [16/200]Step: [300/600] G loss: 3.11203  D loss: 0.17702  \n",
      "Train Epoch: [16/200]Step: [400/600] G loss: 3.99052  D loss: 0.21898  \n",
      "Train Epoch: [16/200]Step: [500/600] G loss: 3.71339  D loss: 0.35938  \n",
      "Train Epoch: [17/200]Step: [0/600]   G loss: 2.71385  D loss: 0.36413  \n",
      "Train Epoch: [17/200]Step: [100/600] G loss: 4.49147  D loss: 0.10857  \n",
      "Train Epoch: [17/200]Step: [200/600] G loss: 3.93864  D loss: 0.16300  \n",
      "Train Epoch: [17/200]Step: [300/600] G loss: 4.45871  D loss: 0.04786  \n",
      "Train Epoch: [17/200]Step: [400/600] G loss: 5.14454  D loss: 0.13107  \n",
      "Train Epoch: [17/200]Step: [500/600] G loss: 3.86720  D loss: 0.14402  \n",
      "Train Epoch: [18/200]Step: [0/600]   G loss: 4.05203  D loss: 0.19996  \n",
      "Train Epoch: [18/200]Step: [100/600] G loss: 3.76233  D loss: 0.11524  \n",
      "Train Epoch: [18/200]Step: [200/600] G loss: 4.38194  D loss: 0.18608  \n",
      "Train Epoch: [18/200]Step: [300/600] G loss: 3.35823  D loss: 0.14273  \n",
      "Train Epoch: [18/200]Step: [400/600] G loss: 4.93721  D loss: 0.10255  \n",
      "Train Epoch: [18/200]Step: [500/600] G loss: 3.59254  D loss: 0.13107  \n",
      "Train Epoch: [19/200]Step: [0/600]   G loss: 4.04861  D loss: 0.14985  \n",
      "Train Epoch: [19/200]Step: [100/600] G loss: 5.01749  D loss: 0.09059  \n",
      "Train Epoch: [19/200]Step: [200/600] G loss: 3.07031  D loss: 0.23836  \n",
      "Train Epoch: [19/200]Step: [300/600] G loss: 4.79259  D loss: 0.10370  \n",
      "Train Epoch: [19/200]Step: [400/600] G loss: 4.88825  D loss: 0.12043  \n",
      "Train Epoch: [19/200]Step: [500/600] G loss: 4.60510  D loss: 0.12721  \n",
      "Train Epoch: [20/200]Step: [0/600]   G loss: 4.46779  D loss: 0.18458  \n",
      "Train Epoch: [20/200]Step: [100/600] G loss: 3.71254  D loss: 0.10490  \n",
      "Train Epoch: [20/200]Step: [200/600] G loss: 4.51031  D loss: 0.13182  \n",
      "Train Epoch: [20/200]Step: [300/600] G loss: 3.84381  D loss: 0.12789  \n",
      "Train Epoch: [20/200]Step: [400/600] G loss: 4.06990  D loss: 0.06808  \n",
      "Train Epoch: [20/200]Step: [500/600] G loss: 4.53907  D loss: 0.11796  \n",
      "Train Epoch: [21/200]Step: [0/600]   G loss: 5.32585  D loss: 0.10964  \n",
      "Train Epoch: [21/200]Step: [100/600] G loss: 5.88791  D loss: 0.06200  \n",
      "Train Epoch: [21/200]Step: [200/600] G loss: 4.48944  D loss: 0.10451  \n",
      "Train Epoch: [21/200]Step: [300/600] G loss: 3.91221  D loss: 0.12372  \n",
      "Train Epoch: [21/200]Step: [400/600] G loss: 5.24350  D loss: 0.12614  \n",
      "Train Epoch: [21/200]Step: [500/600] G loss: 4.56926  D loss: 0.12675  \n",
      "Train Epoch: [22/200]Step: [0/600]   G loss: 3.69376  D loss: 0.17861  \n",
      "Train Epoch: [22/200]Step: [100/600] G loss: 3.77883  D loss: 0.06451  \n",
      "Train Epoch: [22/200]Step: [200/600] G loss: 3.95464  D loss: 0.14199  \n",
      "Train Epoch: [22/200]Step: [300/600] G loss: 5.75917  D loss: 0.10999  \n",
      "Train Epoch: [22/200]Step: [400/600] G loss: 5.86693  D loss: 0.10278  \n",
      "Train Epoch: [22/200]Step: [500/600] G loss: 4.70167  D loss: 0.05517  \n",
      "Train Epoch: [23/200]Step: [0/600]   G loss: 3.86448  D loss: 0.43015  \n",
      "Train Epoch: [23/200]Step: [100/600] G loss: 4.20779  D loss: 0.11903  \n",
      "Train Epoch: [23/200]Step: [200/600] G loss: 4.70437  D loss: 0.05715  \n",
      "Train Epoch: [23/200]Step: [300/600] G loss: 5.12340  D loss: 0.13020  \n",
      "Train Epoch: [23/200]Step: [400/600] G loss: 4.74102  D loss: 0.21718  \n",
      "Train Epoch: [23/200]Step: [500/600] G loss: 5.33125  D loss: 0.15674  \n",
      "Train Epoch: [24/200]Step: [0/600]   G loss: 5.12373  D loss: 0.04227  \n",
      "Train Epoch: [24/200]Step: [100/600] G loss: 5.35768  D loss: 0.04734  \n",
      "Train Epoch: [24/200]Step: [200/600] G loss: 3.45972  D loss: 0.07889  \n",
      "Train Epoch: [24/200]Step: [300/600] G loss: 5.29102  D loss: 0.09600  \n",
      "Train Epoch: [24/200]Step: [400/600] G loss: 5.25438  D loss: 0.03445  \n",
      "Train Epoch: [24/200]Step: [500/600] G loss: 6.16491  D loss: 0.07175  \n",
      "Train Epoch: [25/200]Step: [0/600]   G loss: 4.97600  D loss: 0.08682  \n",
      "Train Epoch: [25/200]Step: [100/600] G loss: 5.58147  D loss: 0.04637  \n",
      "Train Epoch: [25/200]Step: [200/600] G loss: 4.56470  D loss: 0.07782  \n",
      "Train Epoch: [25/200]Step: [300/600] G loss: 5.13510  D loss: 0.04051  \n",
      "Train Epoch: [25/200]Step: [400/600] G loss: 4.90308  D loss: 0.14283  \n",
      "Train Epoch: [25/200]Step: [500/600] G loss: 5.09941  D loss: 0.18659  \n",
      "Train Epoch: [26/200]Step: [0/600]   G loss: 6.10516  D loss: 0.15112  \n",
      "Train Epoch: [26/200]Step: [100/600] G loss: 6.17643  D loss: 0.05022  \n",
      "Train Epoch: [26/200]Step: [200/600] G loss: 3.68799  D loss: 0.09600  \n",
      "Train Epoch: [26/200]Step: [300/600] G loss: 6.67012  D loss: 0.24345  \n",
      "Train Epoch: [26/200]Step: [400/600] G loss: 3.89309  D loss: 0.05681  \n",
      "Train Epoch: [26/200]Step: [500/600] G loss: 6.40463  D loss: 0.07024  \n",
      "Train Epoch: [27/200]Step: [0/600]   G loss: 4.91728  D loss: 0.03419  \n",
      "Train Epoch: [27/200]Step: [100/600] G loss: 6.02219  D loss: 0.02374  \n",
      "Train Epoch: [27/200]Step: [200/600] G loss: 5.25400  D loss: 0.03714  \n",
      "Train Epoch: [27/200]Step: [300/600] G loss: 4.94473  D loss: 0.03572  \n",
      "Train Epoch: [27/200]Step: [400/600] G loss: 6.78045  D loss: 0.04402  \n",
      "Train Epoch: [27/200]Step: [500/600] G loss: 5.07087  D loss: 0.20315  \n",
      "Train Epoch: [28/200]Step: [0/600]   G loss: 4.56722  D loss: 0.05369  \n",
      "Train Epoch: [28/200]Step: [100/600] G loss: 5.37965  D loss: 0.06994  \n",
      "Train Epoch: [28/200]Step: [200/600] G loss: 4.44845  D loss: 0.08525  \n",
      "Train Epoch: [28/200]Step: [300/600] G loss: 6.10439  D loss: 0.04456  \n",
      "Train Epoch: [28/200]Step: [400/600] G loss: 5.57312  D loss: 0.14246  \n",
      "Train Epoch: [28/200]Step: [500/600] G loss: 5.52109  D loss: 0.14657  \n",
      "Train Epoch: [29/200]Step: [0/600]   G loss: 4.79454  D loss: 0.20897  \n",
      "Train Epoch: [29/200]Step: [100/600] G loss: 5.48527  D loss: 0.09395  \n",
      "Train Epoch: [29/200]Step: [200/600] G loss: 6.12040  D loss: 0.06678  \n",
      "Train Epoch: [29/200]Step: [300/600] G loss: 6.03329  D loss: 0.03546  \n",
      "Train Epoch: [29/200]Step: [400/600] G loss: 5.10153  D loss: 0.13580  \n",
      "Train Epoch: [29/200]Step: [500/600] G loss: 5.33272  D loss: 0.08651  \n",
      "Train Epoch: [30/200]Step: [0/600]   G loss: 6.90740  D loss: 0.18908  \n",
      "Train Epoch: [30/200]Step: [100/600] G loss: 5.33046  D loss: 0.02427  \n",
      "Train Epoch: [30/200]Step: [200/600] G loss: 6.11867  D loss: 0.04176  \n",
      "Train Epoch: [30/200]Step: [300/600] G loss: 5.43671  D loss: 0.11660  \n",
      "Train Epoch: [30/200]Step: [400/600] G loss: 5.50236  D loss: 0.06380  \n",
      "Train Epoch: [30/200]Step: [500/600] G loss: 6.44511  D loss: 0.04119  \n",
      "Train Epoch: [31/200]Step: [0/600]   G loss: 4.43512  D loss: 0.10249  \n",
      "Train Epoch: [31/200]Step: [100/600] G loss: 5.51690  D loss: 0.07687  \n",
      "Train Epoch: [31/200]Step: [200/600] G loss: 5.76344  D loss: 0.04763  \n",
      "Train Epoch: [31/200]Step: [300/600] G loss: 5.28728  D loss: 0.14626  \n",
      "Train Epoch: [31/200]Step: [400/600] G loss: 5.45315  D loss: 0.05633  \n",
      "Train Epoch: [31/200]Step: [500/600] G loss: 6.36957  D loss: 0.04357  \n",
      "Train Epoch: [32/200]Step: [0/600]   G loss: 5.69519  D loss: 0.08110  \n",
      "Train Epoch: [32/200]Step: [100/600] G loss: 6.18966  D loss: 0.04243  \n",
      "Train Epoch: [32/200]Step: [200/600] G loss: 6.24334  D loss: 0.00978  \n",
      "Train Epoch: [32/200]Step: [300/600] G loss: 5.27697  D loss: 0.06702  \n",
      "Train Epoch: [32/200]Step: [400/600] G loss: 7.55060  D loss: 0.01814  \n",
      "Train Epoch: [32/200]Step: [500/600] G loss: 4.69399  D loss: 0.18181  \n",
      "Train Epoch: [33/200]Step: [0/600]   G loss: 5.82607  D loss: 0.05298  \n",
      "Train Epoch: [33/200]Step: [100/600] G loss: 7.50699  D loss: 0.05227  \n",
      "Train Epoch: [33/200]Step: [200/600] G loss: 4.99510  D loss: 0.06451  \n",
      "Train Epoch: [33/200]Step: [300/600] G loss: 6.04264  D loss: 0.19313  \n",
      "Train Epoch: [33/200]Step: [400/600] G loss: 5.92024  D loss: 0.01986  \n",
      "Train Epoch: [33/200]Step: [500/600] G loss: 4.10176  D loss: 0.12115  \n",
      "Train Epoch: [34/200]Step: [0/600]   G loss: 6.49760  D loss: 0.03407  \n",
      "Train Epoch: [34/200]Step: [100/600] G loss: 6.68779  D loss: 0.06439  \n",
      "Train Epoch: [34/200]Step: [200/600] G loss: 5.36529  D loss: 0.09538  \n",
      "Train Epoch: [34/200]Step: [300/600] G loss: 5.47592  D loss: 0.02485  \n",
      "Train Epoch: [34/200]Step: [400/600] G loss: 5.83299  D loss: 0.01700  \n",
      "Train Epoch: [34/200]Step: [500/600] G loss: 6.07789  D loss: 0.06250  \n",
      "Train Epoch: [35/200]Step: [0/600]   G loss: 5.49378  D loss: 0.02778  \n",
      "Train Epoch: [35/200]Step: [100/600] G loss: 6.32163  D loss: 0.16036  \n",
      "Train Epoch: [35/200]Step: [200/600] G loss: 5.58525  D loss: 0.02280  \n",
      "Train Epoch: [35/200]Step: [300/600] G loss: 6.28370  D loss: 0.04211  \n",
      "Train Epoch: [35/200]Step: [400/600] G loss: 5.82535  D loss: 0.01974  \n",
      "Train Epoch: [35/200]Step: [500/600] G loss: 4.71654  D loss: 0.05059  \n",
      "Train Epoch: [36/200]Step: [0/600]   G loss: 5.31701  D loss: 0.04729  \n",
      "Train Epoch: [36/200]Step: [100/600] G loss: 5.79772  D loss: 0.03938  \n",
      "Train Epoch: [36/200]Step: [200/600] G loss: 5.00669  D loss: 0.04804  \n",
      "Train Epoch: [36/200]Step: [300/600] G loss: 5.64680  D loss: 0.12825  \n",
      "Train Epoch: [36/200]Step: [400/600] G loss: 5.70991  D loss: 0.12191  \n",
      "Train Epoch: [36/200]Step: [500/600] G loss: 6.23166  D loss: 0.05210  \n",
      "Train Epoch: [37/200]Step: [0/600]   G loss: 6.55945  D loss: 0.01277  \n",
      "Train Epoch: [37/200]Step: [100/600] G loss: 6.19082  D loss: 0.03158  \n",
      "Train Epoch: [37/200]Step: [200/600] G loss: 8.25561  D loss: 0.03674  \n",
      "Train Epoch: [37/200]Step: [300/600] G loss: 6.19008  D loss: 0.02159  \n",
      "Train Epoch: [37/200]Step: [400/600] G loss: 7.12526  D loss: 0.05380  \n",
      "Train Epoch: [37/200]Step: [500/600] G loss: 6.75214  D loss: 0.02655  \n",
      "Train Epoch: [38/200]Step: [0/600]   G loss: 5.78268  D loss: 0.02581  \n",
      "Train Epoch: [38/200]Step: [100/600] G loss: 6.88165  D loss: 0.04394  \n",
      "Train Epoch: [38/200]Step: [200/600] G loss: 8.28726  D loss: 0.01096  \n",
      "Train Epoch: [38/200]Step: [300/600] G loss: 7.58371  D loss: 0.08690  \n",
      "Train Epoch: [38/200]Step: [400/600] G loss: 5.40636  D loss: 0.06322  \n",
      "Train Epoch: [38/200]Step: [500/600] G loss: 5.12497  D loss: 0.04662  \n",
      "Train Epoch: [39/200]Step: [0/600]   G loss: 4.96101  D loss: 0.09689  \n",
      "Train Epoch: [39/200]Step: [100/600] G loss: 6.56338  D loss: 0.05284  \n",
      "Train Epoch: [39/200]Step: [200/600] G loss: 6.32668  D loss: 0.14210  \n",
      "Train Epoch: [39/200]Step: [300/600] G loss: 5.60188  D loss: 0.05646  \n",
      "Train Epoch: [39/200]Step: [400/600] G loss: 5.88126  D loss: 0.02553  \n",
      "Train Epoch: [39/200]Step: [500/600] G loss: 6.15923  D loss: 0.08577  \n",
      "Train Epoch: [40/200]Step: [0/600]   G loss: 6.86139  D loss: 0.06409  \n",
      "Train Epoch: [40/200]Step: [100/600] G loss: 5.88065  D loss: 0.07083  \n",
      "Train Epoch: [40/200]Step: [200/600] G loss: 6.31677  D loss: 0.01411  \n",
      "Train Epoch: [40/200]Step: [300/600] G loss: 6.28346  D loss: 0.03124  \n",
      "Train Epoch: [40/200]Step: [400/600] G loss: 5.46728  D loss: 0.06782  \n",
      "Train Epoch: [40/200]Step: [500/600] G loss: 6.66023  D loss: 0.07323  \n",
      "Train Epoch: [41/200]Step: [0/600]   G loss: 6.34212  D loss: 0.09116  \n",
      "Train Epoch: [41/200]Step: [100/600] G loss: 5.22387  D loss: 0.04179  \n",
      "Train Epoch: [41/200]Step: [200/600] G loss: 5.58680  D loss: 0.06784  \n",
      "Train Epoch: [41/200]Step: [300/600] G loss: 5.93168  D loss: 0.06714  \n",
      "Train Epoch: [41/200]Step: [400/600] G loss: 5.09066  D loss: 0.16948  \n",
      "Train Epoch: [41/200]Step: [500/600] G loss: 6.62920  D loss: 0.10376  \n",
      "Train Epoch: [42/200]Step: [0/600]   G loss: 7.16656  D loss: 0.09642  \n",
      "Train Epoch: [42/200]Step: [100/600] G loss: 4.82717  D loss: 0.08475  \n",
      "Train Epoch: [42/200]Step: [200/600] G loss: 5.44739  D loss: 0.08657  \n",
      "Train Epoch: [42/200]Step: [300/600] G loss: 5.10668  D loss: 0.10662  \n",
      "Train Epoch: [42/200]Step: [400/600] G loss: 5.54882  D loss: 0.02768  \n",
      "Train Epoch: [42/200]Step: [500/600] G loss: 5.74104  D loss: 0.03446  \n",
      "Train Epoch: [43/200]Step: [0/600]   G loss: 6.04860  D loss: 0.19555  \n",
      "Train Epoch: [43/200]Step: [100/600] G loss: 5.77574  D loss: 0.04281  \n",
      "Train Epoch: [43/200]Step: [200/600] G loss: 6.54667  D loss: 0.07050  \n",
      "Train Epoch: [43/200]Step: [300/600] G loss: 6.31291  D loss: 0.12831  \n",
      "Train Epoch: [43/200]Step: [400/600] G loss: 6.22696  D loss: 0.09384  \n",
      "Train Epoch: [43/200]Step: [500/600] G loss: 4.98162  D loss: 0.03456  \n",
      "Train Epoch: [44/200]Step: [0/600]   G loss: 5.14316  D loss: 0.06045  \n",
      "Train Epoch: [44/200]Step: [100/600] G loss: 6.09185  D loss: 0.03128  \n",
      "Train Epoch: [44/200]Step: [200/600] G loss: 6.90148  D loss: 0.14817  \n",
      "Train Epoch: [44/200]Step: [300/600] G loss: 5.10035  D loss: 0.04789  \n",
      "Train Epoch: [44/200]Step: [400/600] G loss: 5.53499  D loss: 0.04904  \n",
      "Train Epoch: [44/200]Step: [500/600] G loss: 5.13564  D loss: 0.02823  \n",
      "Train Epoch: [45/200]Step: [0/600]   G loss: 7.20443  D loss: 0.03021  \n",
      "Train Epoch: [45/200]Step: [100/600] G loss: 8.00418  D loss: 0.05392  \n",
      "Train Epoch: [45/200]Step: [200/600] G loss: 5.36020  D loss: 0.05009  \n",
      "Train Epoch: [45/200]Step: [300/600] G loss: 5.94404  D loss: 0.03142  \n",
      "Train Epoch: [45/200]Step: [400/600] G loss: 6.53377  D loss: 0.05622  \n",
      "Train Epoch: [45/200]Step: [500/600] G loss: 6.52799  D loss: 0.07888  \n",
      "Train Epoch: [46/200]Step: [0/600]   G loss: 5.96214  D loss: 0.05266  \n",
      "Train Epoch: [46/200]Step: [100/600] G loss: 5.09541  D loss: 0.14960  \n",
      "Train Epoch: [46/200]Step: [200/600] G loss: 5.12493  D loss: 0.03715  \n",
      "Train Epoch: [46/200]Step: [300/600] G loss: 5.51193  D loss: 0.02148  \n",
      "Train Epoch: [46/200]Step: [400/600] G loss: 6.19029  D loss: 0.03845  \n",
      "Train Epoch: [46/200]Step: [500/600] G loss: 7.49582  D loss: 0.30950  \n",
      "Train Epoch: [47/200]Step: [0/600]   G loss: 6.18857  D loss: 0.02040  \n",
      "Train Epoch: [47/200]Step: [100/600] G loss: 4.74543  D loss: 0.04872  \n",
      "Train Epoch: [47/200]Step: [200/600] G loss: 5.70407  D loss: 0.08968  \n",
      "Train Epoch: [47/200]Step: [300/600] G loss: 5.33595  D loss: 0.09212  \n",
      "Train Epoch: [47/200]Step: [400/600] G loss: 7.98938  D loss: 0.03189  \n",
      "Train Epoch: [47/200]Step: [500/600] G loss: 5.61686  D loss: 0.13261  \n",
      "Train Epoch: [48/200]Step: [0/600]   G loss: 7.46642  D loss: 0.04456  \n",
      "Train Epoch: [48/200]Step: [100/600] G loss: 7.15693  D loss: 0.00737  \n",
      "Train Epoch: [48/200]Step: [200/600] G loss: 6.12438  D loss: 0.07692  \n",
      "Train Epoch: [48/200]Step: [300/600] G loss: 5.80722  D loss: 0.06204  \n",
      "Train Epoch: [48/200]Step: [400/600] G loss: 6.91180  D loss: 0.02615  \n",
      "Train Epoch: [48/200]Step: [500/600] G loss: 5.94959  D loss: 0.07575  \n",
      "Train Epoch: [49/200]Step: [0/600]   G loss: 6.10028  D loss: 0.05125  \n",
      "Train Epoch: [49/200]Step: [100/600] G loss: 6.15734  D loss: 0.05971  \n",
      "Train Epoch: [49/200]Step: [200/600] G loss: 6.21543  D loss: 0.08120  \n",
      "Train Epoch: [49/200]Step: [300/600] G loss: 6.67984  D loss: 0.02618  \n",
      "Train Epoch: [49/200]Step: [400/600] G loss: 4.76203  D loss: 0.12975  \n",
      "Train Epoch: [49/200]Step: [500/600] G loss: 6.18676  D loss: 0.04433  \n",
      "Train Epoch: [50/200]Step: [0/600]   G loss: 6.42184  D loss: 0.00842  \n",
      "Train Epoch: [50/200]Step: [100/600] G loss: 7.28391  D loss: 0.01755  \n",
      "Train Epoch: [50/200]Step: [200/600] G loss: 5.94761  D loss: 0.02267  \n",
      "Train Epoch: [50/200]Step: [300/600] G loss: 6.13893  D loss: 0.05055  \n",
      "Train Epoch: [50/200]Step: [400/600] G loss: 5.87327  D loss: 0.05723  \n",
      "Train Epoch: [50/200]Step: [500/600] G loss: 6.28561  D loss: 0.02720  \n",
      "Train Epoch: [51/200]Step: [0/600]   G loss: 7.29709  D loss: 0.03010  \n",
      "Train Epoch: [51/200]Step: [100/600] G loss: 6.06225  D loss: 0.01903  \n",
      "Train Epoch: [51/200]Step: [200/600] G loss: 6.83717  D loss: 0.03554  \n",
      "Train Epoch: [51/200]Step: [300/600] G loss: 6.65032  D loss: 0.13192  \n",
      "Train Epoch: [51/200]Step: [400/600] G loss: 6.28894  D loss: 0.03570  \n",
      "Train Epoch: [51/200]Step: [500/600] G loss: 6.29938  D loss: 0.04921  \n",
      "Train Epoch: [52/200]Step: [0/600]   G loss: 5.63689  D loss: 0.08495  \n",
      "Train Epoch: [52/200]Step: [100/600] G loss: 5.21625  D loss: 0.05102  \n",
      "Train Epoch: [52/200]Step: [200/600] G loss: 5.66554  D loss: 0.15745  \n",
      "Train Epoch: [52/200]Step: [300/600] G loss: 6.23853  D loss: 0.08642  \n",
      "Train Epoch: [52/200]Step: [400/600] G loss: 7.74362  D loss: 0.14693  \n",
      "Train Epoch: [52/200]Step: [500/600] G loss: 5.65927  D loss: 0.15928  \n",
      "Train Epoch: [53/200]Step: [0/600]   G loss: 6.67467  D loss: 0.03837  \n",
      "Train Epoch: [53/200]Step: [100/600] G loss: 3.75296  D loss: 0.20875  \n",
      "Train Epoch: [53/200]Step: [200/600] G loss: 6.10069  D loss: 0.01912  \n",
      "Train Epoch: [53/200]Step: [300/600] G loss: 7.29680  D loss: 0.06355  \n",
      "Train Epoch: [53/200]Step: [400/600] G loss: 5.71404  D loss: 0.04617  \n",
      "Train Epoch: [53/200]Step: [500/600] G loss: 6.84817  D loss: 0.09641  \n",
      "Train Epoch: [54/200]Step: [0/600]   G loss: 6.95063  D loss: 0.16668  \n",
      "Train Epoch: [54/200]Step: [100/600] G loss: 6.04318  D loss: 0.08424  \n",
      "Train Epoch: [54/200]Step: [200/600] G loss: 6.48200  D loss: 0.08287  \n",
      "Train Epoch: [54/200]Step: [300/600] G loss: 6.83210  D loss: 0.06767  \n",
      "Train Epoch: [54/200]Step: [400/600] G loss: 7.14076  D loss: 0.03483  \n",
      "Train Epoch: [54/200]Step: [500/600] G loss: 6.25826  D loss: 0.24760  \n",
      "Train Epoch: [55/200]Step: [0/600]   G loss: 7.88177  D loss: 0.02614  \n",
      "Train Epoch: [55/200]Step: [100/600] G loss: 5.67085  D loss: 0.07667  \n",
      "Train Epoch: [55/200]Step: [200/600] G loss: 4.29928  D loss: 0.13874  \n",
      "Train Epoch: [55/200]Step: [300/600] G loss: 7.07821  D loss: 0.02815  \n",
      "Train Epoch: [55/200]Step: [400/600] G loss: 6.69543  D loss: 0.05262  \n",
      "Train Epoch: [55/200]Step: [500/600] G loss: 7.00859  D loss: 0.10340  \n",
      "Train Epoch: [56/200]Step: [0/600]   G loss: 7.32047  D loss: 0.03755  \n",
      "Train Epoch: [56/200]Step: [100/600] G loss: 6.63608  D loss: 0.08722  \n",
      "Train Epoch: [56/200]Step: [200/600] G loss: 4.39035  D loss: 0.12259  \n",
      "Train Epoch: [56/200]Step: [300/600] G loss: 6.60030  D loss: 0.02316  \n",
      "Train Epoch: [56/200]Step: [400/600] G loss: 7.03463  D loss: 0.02055  \n",
      "Train Epoch: [56/200]Step: [500/600] G loss: 6.04184  D loss: 0.05069  \n",
      "Train Epoch: [57/200]Step: [0/600]   G loss: 5.60335  D loss: 0.03325  \n",
      "Train Epoch: [57/200]Step: [100/600] G loss: 6.75689  D loss: 0.12528  \n",
      "Train Epoch: [57/200]Step: [200/600] G loss: 5.89025  D loss: 0.06762  \n",
      "Train Epoch: [57/200]Step: [300/600] G loss: 5.84221  D loss: 0.03430  \n",
      "Train Epoch: [57/200]Step: [400/600] G loss: 7.66228  D loss: 0.12058  \n",
      "Train Epoch: [57/200]Step: [500/600] G loss: 6.69449  D loss: 0.01119  \n",
      "Train Epoch: [58/200]Step: [0/600]   G loss: 6.57975  D loss: 0.07453  \n",
      "Train Epoch: [58/200]Step: [100/600] G loss: 7.23081  D loss: 0.12597  \n",
      "Train Epoch: [58/200]Step: [200/600] G loss: 6.06484  D loss: 0.03535  \n",
      "Train Epoch: [58/200]Step: [300/600] G loss: 5.00954  D loss: 0.06854  \n",
      "Train Epoch: [58/200]Step: [400/600] G loss: 5.25293  D loss: 0.09023  \n",
      "Train Epoch: [58/200]Step: [500/600] G loss: 7.08804  D loss: 0.06265  \n",
      "Train Epoch: [59/200]Step: [0/600]   G loss: 5.70526  D loss: 0.03604  \n",
      "Train Epoch: [59/200]Step: [100/600] G loss: 6.29491  D loss: 0.06144  \n",
      "Train Epoch: [59/200]Step: [200/600] G loss: 6.32354  D loss: 0.11142  \n",
      "Train Epoch: [59/200]Step: [300/600] G loss: 6.31652  D loss: 0.22858  \n",
      "Train Epoch: [59/200]Step: [400/600] G loss: 5.50901  D loss: 0.11154  \n",
      "Train Epoch: [59/200]Step: [500/600] G loss: 6.94637  D loss: 0.07646  \n",
      "Train Epoch: [60/200]Step: [0/600]   G loss: 5.86411  D loss: 0.04944  \n",
      "Train Epoch: [60/200]Step: [100/600] G loss: 6.87503  D loss: 0.01259  \n",
      "Train Epoch: [60/200]Step: [200/600] G loss: 5.37645  D loss: 0.02950  \n",
      "Train Epoch: [60/200]Step: [300/600] G loss: 5.32783  D loss: 0.19871  \n",
      "Train Epoch: [60/200]Step: [400/600] G loss: 6.32788  D loss: 0.10862  \n",
      "Train Epoch: [60/200]Step: [500/600] G loss: 6.29266  D loss: 0.06329  \n",
      "Train Epoch: [61/200]Step: [0/600]   G loss: 7.01956  D loss: 0.21186  \n",
      "Train Epoch: [61/200]Step: [100/600] G loss: 4.86070  D loss: 0.08000  \n",
      "Train Epoch: [61/200]Step: [200/600] G loss: 6.16009  D loss: 0.06515  \n",
      "Train Epoch: [61/200]Step: [300/600] G loss: 5.47431  D loss: 0.12179  \n",
      "Train Epoch: [61/200]Step: [400/600] G loss: 4.38968  D loss: 0.24389  \n",
      "Train Epoch: [61/200]Step: [500/600] G loss: 7.07805  D loss: 0.19632  \n",
      "Train Epoch: [62/200]Step: [0/600]   G loss: 4.96068  D loss: 0.20410  \n",
      "Train Epoch: [62/200]Step: [100/600] G loss: 5.95166  D loss: 0.03080  \n",
      "Train Epoch: [62/200]Step: [200/600] G loss: 6.50760  D loss: 0.08038  \n",
      "Train Epoch: [62/200]Step: [300/600] G loss: 5.43224  D loss: 0.06948  \n",
      "Train Epoch: [62/200]Step: [400/600] G loss: 4.85884  D loss: 0.12346  \n",
      "Train Epoch: [62/200]Step: [500/600] G loss: 7.61540  D loss: 0.06780  \n",
      "Train Epoch: [63/200]Step: [0/600]   G loss: 6.31482  D loss: 0.04783  \n",
      "Train Epoch: [63/200]Step: [100/600] G loss: 5.32875  D loss: 0.03340  \n",
      "Train Epoch: [63/200]Step: [200/600] G loss: 5.38017  D loss: 0.06339  \n",
      "Train Epoch: [63/200]Step: [300/600] G loss: 7.60757  D loss: 0.02197  \n",
      "Train Epoch: [63/200]Step: [400/600] G loss: 6.56668  D loss: 0.07047  \n",
      "Train Epoch: [63/200]Step: [500/600] G loss: 4.72134  D loss: 0.06712  \n",
      "Train Epoch: [64/200]Step: [0/600]   G loss: 5.72129  D loss: 0.05796  \n",
      "Train Epoch: [64/200]Step: [100/600] G loss: 4.88149  D loss: 0.06127  \n",
      "Train Epoch: [64/200]Step: [200/600] G loss: 7.12497  D loss: 0.03125  \n",
      "Train Epoch: [64/200]Step: [300/600] G loss: 5.40724  D loss: 0.04346  \n",
      "Train Epoch: [64/200]Step: [400/600] G loss: 6.14362  D loss: 0.37540  \n",
      "Train Epoch: [64/200]Step: [500/600] G loss: 7.51854  D loss: 0.02198  \n",
      "Train Epoch: [65/200]Step: [0/600]   G loss: 7.17508  D loss: 0.06251  \n",
      "Train Epoch: [65/200]Step: [100/600] G loss: 5.22259  D loss: 0.10468  \n",
      "Train Epoch: [65/200]Step: [200/600] G loss: 5.61905  D loss: 0.05307  \n",
      "Train Epoch: [65/200]Step: [300/600] G loss: 6.38245  D loss: 0.01384  \n",
      "Train Epoch: [65/200]Step: [400/600] G loss: 6.28933  D loss: 0.07427  \n",
      "Train Epoch: [65/200]Step: [500/600] G loss: 5.77016  D loss: 0.06014  \n",
      "Train Epoch: [66/200]Step: [0/600]   G loss: 5.73565  D loss: 0.01705  \n",
      "Train Epoch: [66/200]Step: [100/600] G loss: 5.76757  D loss: 0.18278  \n",
      "Train Epoch: [66/200]Step: [200/600] G loss: 7.35754  D loss: 0.01376  \n",
      "Train Epoch: [66/200]Step: [300/600] G loss: 7.27077  D loss: 0.08205  \n",
      "Train Epoch: [66/200]Step: [400/600] G loss: 5.97364  D loss: 0.04155  \n",
      "Train Epoch: [66/200]Step: [500/600] G loss: 7.57593  D loss: 0.03469  \n",
      "Train Epoch: [67/200]Step: [0/600]   G loss: 6.61181  D loss: 0.06448  \n",
      "Train Epoch: [67/200]Step: [100/600] G loss: 6.94126  D loss: 0.06966  \n",
      "Train Epoch: [67/200]Step: [200/600] G loss: 4.98661  D loss: 0.03273  \n",
      "Train Epoch: [67/200]Step: [300/600] G loss: 6.84527  D loss: 0.02625  \n",
      "Train Epoch: [67/200]Step: [400/600] G loss: 7.17477  D loss: 0.06078  \n",
      "Train Epoch: [67/200]Step: [500/600] G loss: 7.51697  D loss: 0.08221  \n",
      "Train Epoch: [68/200]Step: [0/600]   G loss: 7.63942  D loss: 0.00861  \n",
      "Train Epoch: [68/200]Step: [100/600] G loss: 5.92486  D loss: 0.18099  \n",
      "Train Epoch: [68/200]Step: [200/600] G loss: 4.94969  D loss: 0.37956  \n",
      "Train Epoch: [68/200]Step: [300/600] G loss: 6.81739  D loss: 0.08570  \n",
      "Train Epoch: [68/200]Step: [400/600] G loss: 6.44893  D loss: 0.05711  \n",
      "Train Epoch: [68/200]Step: [500/600] G loss: 6.04787  D loss: 0.02786  \n",
      "Train Epoch: [69/200]Step: [0/600]   G loss: 5.92245  D loss: 0.03396  \n",
      "Train Epoch: [69/200]Step: [100/600] G loss: 6.22269  D loss: 0.10069  \n",
      "Train Epoch: [69/200]Step: [200/600] G loss: 7.43131  D loss: 0.02015  \n",
      "Train Epoch: [69/200]Step: [300/600] G loss: 5.53926  D loss: 0.01857  \n",
      "Train Epoch: [69/200]Step: [400/600] G loss: 5.33444  D loss: 0.05140  \n",
      "Train Epoch: [69/200]Step: [500/600] G loss: 7.42098  D loss: 0.02507  \n",
      "Train Epoch: [70/200]Step: [0/600]   G loss: 7.79045  D loss: 0.04271  \n",
      "Train Epoch: [70/200]Step: [100/600] G loss: 5.38076  D loss: 0.02745  \n",
      "Train Epoch: [70/200]Step: [200/600] G loss: 6.77256  D loss: 0.15869  \n",
      "Train Epoch: [70/200]Step: [300/600] G loss: 6.63012  D loss: 0.07598  \n",
      "Train Epoch: [70/200]Step: [400/600] G loss: 6.04841  D loss: 0.06172  \n",
      "Train Epoch: [70/200]Step: [500/600] G loss: 5.74735  D loss: 0.21916  \n",
      "Train Epoch: [71/200]Step: [0/600]   G loss: 4.91325  D loss: 0.02282  \n",
      "Train Epoch: [71/200]Step: [100/600] G loss: 5.44741  D loss: 0.14170  \n",
      "Train Epoch: [71/200]Step: [200/600] G loss: 6.46931  D loss: 0.06416  \n",
      "Train Epoch: [71/200]Step: [300/600] G loss: 5.89088  D loss: 0.07548  \n",
      "Train Epoch: [71/200]Step: [400/600] G loss: 6.68686  D loss: 0.11157  \n",
      "Train Epoch: [71/200]Step: [500/600] G loss: 6.21833  D loss: 0.08099  \n",
      "Train Epoch: [72/200]Step: [0/600]   G loss: 7.65395  D loss: 0.03243  \n",
      "Train Epoch: [72/200]Step: [100/600] G loss: 5.71660  D loss: 0.04273  \n",
      "Train Epoch: [72/200]Step: [200/600] G loss: 8.47882  D loss: 0.02569  \n",
      "Train Epoch: [72/200]Step: [300/600] G loss: 5.45681  D loss: 0.04703  \n",
      "Train Epoch: [72/200]Step: [400/600] G loss: 6.30595  D loss: 0.03558  \n",
      "Train Epoch: [72/200]Step: [500/600] G loss: 7.18996  D loss: 0.09110  \n",
      "Train Epoch: [73/200]Step: [0/600]   G loss: 6.13245  D loss: 0.04454  \n",
      "Train Epoch: [73/200]Step: [100/600] G loss: 6.99402  D loss: 0.06448  \n",
      "Train Epoch: [73/200]Step: [200/600] G loss: 6.87956  D loss: 0.01144  \n",
      "Train Epoch: [73/200]Step: [300/600] G loss: 5.83181  D loss: 0.07614  \n",
      "Train Epoch: [73/200]Step: [400/600] G loss: 5.87985  D loss: 0.16069  \n",
      "Train Epoch: [73/200]Step: [500/600] G loss: 5.09254  D loss: 0.06251  \n",
      "Train Epoch: [74/200]Step: [0/600]   G loss: 6.06635  D loss: 0.05568  \n",
      "Train Epoch: [74/200]Step: [100/600] G loss: 5.10039  D loss: 0.02649  \n",
      "Train Epoch: [74/200]Step: [200/600] G loss: 6.88238  D loss: 0.05561  \n",
      "Train Epoch: [74/200]Step: [300/600] G loss: 7.28922  D loss: 0.03217  \n",
      "Train Epoch: [74/200]Step: [400/600] G loss: 6.26044  D loss: 0.09770  \n",
      "Train Epoch: [74/200]Step: [500/600] G loss: 6.54110  D loss: 0.18793  \n",
      "Train Epoch: [75/200]Step: [0/600]   G loss: 6.49414  D loss: 0.11106  \n",
      "Train Epoch: [75/200]Step: [100/600] G loss: 6.54679  D loss: 0.05866  \n",
      "Train Epoch: [75/200]Step: [200/600] G loss: 8.41014  D loss: 0.01065  \n",
      "Train Epoch: [75/200]Step: [300/600] G loss: 6.60692  D loss: 0.01772  \n",
      "Train Epoch: [75/200]Step: [400/600] G loss: 7.32570  D loss: 0.08363  \n",
      "Train Epoch: [75/200]Step: [500/600] G loss: 6.06249  D loss: 0.06794  \n",
      "Train Epoch: [76/200]Step: [0/600]   G loss: 5.25207  D loss: 0.09743  \n",
      "Train Epoch: [76/200]Step: [100/600] G loss: 5.02450  D loss: 0.14332  \n",
      "Train Epoch: [76/200]Step: [200/600] G loss: 4.84345  D loss: 0.04878  \n",
      "Train Epoch: [76/200]Step: [300/600] G loss: 5.48774  D loss: 0.05957  \n",
      "Train Epoch: [76/200]Step: [400/600] G loss: 6.16063  D loss: 0.00918  \n",
      "Train Epoch: [76/200]Step: [500/600] G loss: 5.37640  D loss: 0.09060  \n",
      "Train Epoch: [77/200]Step: [0/600]   G loss: 6.51974  D loss: 0.04471  \n",
      "Train Epoch: [77/200]Step: [100/600] G loss: 6.45587  D loss: 0.07098  \n",
      "Train Epoch: [77/200]Step: [200/600] G loss: 6.34860  D loss: 0.06247  \n",
      "Train Epoch: [77/200]Step: [300/600] G loss: 5.80125  D loss: 0.05461  \n",
      "Train Epoch: [77/200]Step: [400/600] G loss: 6.01188  D loss: 0.04563  \n",
      "Train Epoch: [77/200]Step: [500/600] G loss: 5.79791  D loss: 0.12961  \n",
      "Train Epoch: [78/200]Step: [0/600]   G loss: 4.87815  D loss: 0.04653  \n",
      "Train Epoch: [78/200]Step: [100/600] G loss: 5.33219  D loss: 0.03913  \n",
      "Train Epoch: [78/200]Step: [200/600] G loss: 4.74471  D loss: 0.06038  \n",
      "Train Epoch: [78/200]Step: [300/600] G loss: 7.63617  D loss: 0.00857  \n",
      "Train Epoch: [78/200]Step: [400/600] G loss: 7.10855  D loss: 0.05957  \n",
      "Train Epoch: [78/200]Step: [500/600] G loss: 6.44000  D loss: 0.06894  \n",
      "Train Epoch: [79/200]Step: [0/600]   G loss: 8.06593  D loss: 0.01648  \n",
      "Train Epoch: [79/200]Step: [100/600] G loss: 6.35771  D loss: 0.05186  \n",
      "Train Epoch: [79/200]Step: [200/600] G loss: 7.15290  D loss: 0.12231  \n",
      "Train Epoch: [79/200]Step: [300/600] G loss: 6.88279  D loss: 0.10192  \n",
      "Train Epoch: [79/200]Step: [400/600] G loss: 7.86545  D loss: 0.07609  \n",
      "Train Epoch: [79/200]Step: [500/600] G loss: 7.82772  D loss: 0.05883  \n",
      "Train Epoch: [80/200]Step: [0/600]   G loss: 8.38872  D loss: 0.06204  \n",
      "Train Epoch: [80/200]Step: [100/600] G loss: 5.24232  D loss: 0.07490  \n",
      "Train Epoch: [80/200]Step: [200/600] G loss: 7.15617  D loss: 0.01347  \n",
      "Train Epoch: [80/200]Step: [300/600] G loss: 5.61043  D loss: 0.12596  \n",
      "Train Epoch: [80/200]Step: [400/600] G loss: 7.25576  D loss: 0.08629  \n",
      "Train Epoch: [80/200]Step: [500/600] G loss: 6.69264  D loss: 0.05496  \n",
      "Train Epoch: [81/200]Step: [0/600]   G loss: 5.43648  D loss: 0.02996  \n",
      "Train Epoch: [81/200]Step: [100/600] G loss: 6.24436  D loss: 0.03462  \n",
      "Train Epoch: [81/200]Step: [200/600] G loss: 6.23704  D loss: 0.09881  \n",
      "Train Epoch: [81/200]Step: [300/600] G loss: 6.67454  D loss: 0.00840  \n",
      "Train Epoch: [81/200]Step: [400/600] G loss: 6.57279  D loss: 0.07136  \n",
      "Train Epoch: [81/200]Step: [500/600] G loss: 5.57656  D loss: 0.13511  \n",
      "Train Epoch: [82/200]Step: [0/600]   G loss: 6.26669  D loss: 0.16919  \n",
      "Train Epoch: [82/200]Step: [100/600] G loss: 5.46966  D loss: 0.04460  \n",
      "Train Epoch: [82/200]Step: [200/600] G loss: 6.46892  D loss: 0.03362  \n",
      "Train Epoch: [82/200]Step: [300/600] G loss: 4.81221  D loss: 0.15039  \n",
      "Train Epoch: [82/200]Step: [400/600] G loss: 6.85188  D loss: 0.08267  \n",
      "Train Epoch: [82/200]Step: [500/600] G loss: 7.36700  D loss: 0.02855  \n",
      "Train Epoch: [83/200]Step: [0/600]   G loss: 8.88317  D loss: 0.12190  \n",
      "Train Epoch: [83/200]Step: [100/600] G loss: 6.43150  D loss: 0.04154  \n",
      "Train Epoch: [83/200]Step: [200/600] G loss: 6.34014  D loss: 0.07692  \n",
      "Train Epoch: [83/200]Step: [300/600] G loss: 5.64301  D loss: 0.01678  \n",
      "Train Epoch: [83/200]Step: [400/600] G loss: 6.15888  D loss: 0.13604  \n",
      "Train Epoch: [83/200]Step: [500/600] G loss: 5.62616  D loss: 0.06053  \n",
      "Train Epoch: [84/200]Step: [0/600]   G loss: 6.48899  D loss: 0.04793  \n",
      "Train Epoch: [84/200]Step: [100/600] G loss: 7.24845  D loss: 0.11240  \n",
      "Train Epoch: [84/200]Step: [200/600] G loss: 6.04860  D loss: 0.12559  \n",
      "Train Epoch: [84/200]Step: [300/600] G loss: 8.95909  D loss: 0.01325  \n",
      "Train Epoch: [84/200]Step: [400/600] G loss: 6.35515  D loss: 0.01548  \n",
      "Train Epoch: [84/200]Step: [500/600] G loss: 6.00975  D loss: 0.03741  \n",
      "Train Epoch: [85/200]Step: [0/600]   G loss: 4.91799  D loss: 0.07873  \n",
      "Train Epoch: [85/200]Step: [100/600] G loss: 5.16279  D loss: 0.02111  \n",
      "Train Epoch: [85/200]Step: [200/600] G loss: 6.06983  D loss: 0.05134  \n",
      "Train Epoch: [85/200]Step: [300/600] G loss: 5.93291  D loss: 0.04071  \n",
      "Train Epoch: [85/200]Step: [400/600] G loss: 5.92136  D loss: 0.17578  \n",
      "Train Epoch: [85/200]Step: [500/600] G loss: 6.99342  D loss: 0.08342  \n",
      "Train Epoch: [86/200]Step: [0/600]   G loss: 5.81235  D loss: 0.03061  \n",
      "Train Epoch: [86/200]Step: [100/600] G loss: 6.91534  D loss: 0.27129  \n",
      "Train Epoch: [86/200]Step: [200/600] G loss: 7.18102  D loss: 0.11209  \n",
      "Train Epoch: [86/200]Step: [300/600] G loss: 6.03128  D loss: 0.06225  \n",
      "Train Epoch: [86/200]Step: [400/600] G loss: 6.42350  D loss: 0.03173  \n",
      "Train Epoch: [86/200]Step: [500/600] G loss: 6.48378  D loss: 0.07397  \n",
      "Train Epoch: [87/200]Step: [0/600]   G loss: 7.57083  D loss: 0.02866  \n",
      "Train Epoch: [87/200]Step: [100/600] G loss: 6.72866  D loss: 0.10621  \n",
      "Train Epoch: [87/200]Step: [200/600] G loss: 7.10650  D loss: 0.04218  \n",
      "Train Epoch: [87/200]Step: [300/600] G loss: 8.31233  D loss: 0.15861  \n",
      "Train Epoch: [87/200]Step: [400/600] G loss: 5.90721  D loss: 0.07162  \n",
      "Train Epoch: [87/200]Step: [500/600] G loss: 5.90431  D loss: 0.13275  \n",
      "Train Epoch: [88/200]Step: [0/600]   G loss: 6.59697  D loss: 0.08169  \n",
      "Train Epoch: [88/200]Step: [100/600] G loss: 5.46756  D loss: 0.22667  \n",
      "Train Epoch: [88/200]Step: [200/600] G loss: 6.72176  D loss: 0.03465  \n",
      "Train Epoch: [88/200]Step: [300/600] G loss: 5.87108  D loss: 0.08618  \n",
      "Train Epoch: [88/200]Step: [400/600] G loss: 5.82581  D loss: 0.17627  \n",
      "Train Epoch: [88/200]Step: [500/600] G loss: 5.15877  D loss: 0.12439  \n",
      "Train Epoch: [89/200]Step: [0/600]   G loss: 5.09187  D loss: 0.02558  \n",
      "Train Epoch: [89/200]Step: [100/600] G loss: 5.05652  D loss: 0.06100  \n",
      "Train Epoch: [89/200]Step: [200/600] G loss: 5.70667  D loss: 0.02074  \n",
      "Train Epoch: [89/200]Step: [300/600] G loss: 5.12433  D loss: 0.15919  \n",
      "Train Epoch: [89/200]Step: [400/600] G loss: 6.34936  D loss: 0.00966  \n",
      "Train Epoch: [89/200]Step: [500/600] G loss: 5.39613  D loss: 0.03528  \n",
      "Train Epoch: [90/200]Step: [0/600]   G loss: 5.93309  D loss: 0.04472  \n",
      "Train Epoch: [90/200]Step: [100/600] G loss: 6.14183  D loss: 0.04885  \n",
      "Train Epoch: [90/200]Step: [200/600] G loss: 4.67835  D loss: 0.13351  \n",
      "Train Epoch: [90/200]Step: [300/600] G loss: 6.67618  D loss: 0.05141  \n",
      "Train Epoch: [90/200]Step: [400/600] G loss: 7.75587  D loss: 0.05141  \n",
      "Train Epoch: [90/200]Step: [500/600] G loss: 5.65706  D loss: 0.05842  \n",
      "Train Epoch: [91/200]Step: [0/600]   G loss: 6.67493  D loss: 0.04886  \n",
      "Train Epoch: [91/200]Step: [100/600] G loss: 7.79315  D loss: 0.13841  \n",
      "Train Epoch: [91/200]Step: [200/600] G loss: 6.69529  D loss: 0.12027  \n",
      "Train Epoch: [91/200]Step: [300/600] G loss: 7.07691  D loss: 0.03095  \n",
      "Train Epoch: [91/200]Step: [400/600] G loss: 7.92581  D loss: 0.21778  \n",
      "Train Epoch: [91/200]Step: [500/600] G loss: 5.76211  D loss: 0.01821  \n",
      "Train Epoch: [92/200]Step: [0/600]   G loss: 5.33002  D loss: 0.16279  \n",
      "Train Epoch: [92/200]Step: [100/600] G loss: 6.39972  D loss: 0.03026  \n",
      "Train Epoch: [92/200]Step: [200/600] G loss: 7.20707  D loss: 0.11303  \n",
      "Train Epoch: [92/200]Step: [300/600] G loss: 6.29555  D loss: 0.08969  \n",
      "Train Epoch: [92/200]Step: [400/600] G loss: 5.39121  D loss: 0.10996  \n",
      "Train Epoch: [92/200]Step: [500/600] G loss: 5.55196  D loss: 0.05087  \n",
      "Train Epoch: [93/200]Step: [0/600]   G loss: 6.70858  D loss: 0.21916  \n",
      "Train Epoch: [93/200]Step: [100/600] G loss: 5.82021  D loss: 0.02275  \n",
      "Train Epoch: [93/200]Step: [200/600] G loss: 5.85611  D loss: 0.01258  \n",
      "Train Epoch: [93/200]Step: [300/600] G loss: 6.14754  D loss: 0.09487  \n",
      "Train Epoch: [93/200]Step: [400/600] G loss: 6.31964  D loss: 0.01971  \n",
      "Train Epoch: [93/200]Step: [500/600] G loss: 6.02364  D loss: 0.09657  \n",
      "Train Epoch: [94/200]Step: [0/600]   G loss: 7.89960  D loss: 0.13315  \n",
      "Train Epoch: [94/200]Step: [100/600] G loss: 8.45168  D loss: 0.05221  \n",
      "Train Epoch: [94/200]Step: [200/600] G loss: 6.08752  D loss: 0.07828  \n",
      "Train Epoch: [94/200]Step: [300/600] G loss: 5.39512  D loss: 0.23509  \n",
      "Train Epoch: [94/200]Step: [400/600] G loss: 6.45907  D loss: 0.16948  \n",
      "Train Epoch: [94/200]Step: [500/600] G loss: 6.70384  D loss: 0.08581  \n",
      "Train Epoch: [95/200]Step: [0/600]   G loss: 6.36442  D loss: 0.05763  \n",
      "Train Epoch: [95/200]Step: [100/600] G loss: 4.60953  D loss: 0.13507  \n",
      "Train Epoch: [95/200]Step: [200/600] G loss: 5.22800  D loss: 0.17381  \n",
      "Train Epoch: [95/200]Step: [300/600] G loss: 6.37025  D loss: 0.04665  \n",
      "Train Epoch: [95/200]Step: [400/600] G loss: 5.31918  D loss: 0.10120  \n",
      "Train Epoch: [95/200]Step: [500/600] G loss: 5.73733  D loss: 0.09402  \n",
      "Train Epoch: [96/200]Step: [0/600]   G loss: 4.27310  D loss: 0.11189  \n",
      "Train Epoch: [96/200]Step: [100/600] G loss: 6.00731  D loss: 0.06521  \n",
      "Train Epoch: [96/200]Step: [200/600] G loss: 6.00786  D loss: 0.01363  \n",
      "Train Epoch: [96/200]Step: [300/600] G loss: 5.78085  D loss: 0.04205  \n",
      "Train Epoch: [96/200]Step: [400/600] G loss: 6.14874  D loss: 0.04241  \n",
      "Train Epoch: [96/200]Step: [500/600] G loss: 5.47349  D loss: 0.02261  \n",
      "Train Epoch: [97/200]Step: [0/600]   G loss: 5.81851  D loss: 0.03220  \n",
      "Train Epoch: [97/200]Step: [100/600] G loss: 6.98425  D loss: 0.06063  \n",
      "Train Epoch: [97/200]Step: [200/600] G loss: 6.63107  D loss: 0.04405  \n",
      "Train Epoch: [97/200]Step: [300/600] G loss: 7.04216  D loss: 0.17895  \n",
      "Train Epoch: [97/200]Step: [400/600] G loss: 5.65126  D loss: 0.10383  \n",
      "Train Epoch: [97/200]Step: [500/600] G loss: 7.78851  D loss: 0.13673  \n",
      "Train Epoch: [98/200]Step: [0/600]   G loss: 7.72343  D loss: 0.22900  \n",
      "Train Epoch: [98/200]Step: [100/600] G loss: 6.22153  D loss: 0.05288  \n",
      "Train Epoch: [98/200]Step: [200/600] G loss: 7.18514  D loss: 0.04972  \n",
      "Train Epoch: [98/200]Step: [300/600] G loss: 6.09896  D loss: 0.11300  \n",
      "Train Epoch: [98/200]Step: [400/600] G loss: 5.24910  D loss: 0.19172  \n",
      "Train Epoch: [98/200]Step: [500/600] G loss: 6.47261  D loss: 0.09952  \n",
      "Train Epoch: [99/200]Step: [0/600]   G loss: 5.94324  D loss: 0.11512  \n",
      "Train Epoch: [99/200]Step: [100/600] G loss: 6.59500  D loss: 0.06538  \n",
      "Train Epoch: [99/200]Step: [200/600] G loss: 6.47519  D loss: 0.04914  \n",
      "Train Epoch: [99/200]Step: [300/600] G loss: 5.53377  D loss: 0.07714  \n",
      "Train Epoch: [99/200]Step: [400/600] G loss: 5.45401  D loss: 0.01667  \n",
      "Train Epoch: [99/200]Step: [500/600] G loss: 4.99448  D loss: 0.08729  \n",
      "Train Epoch: [100/200]Step: [0/600]   G loss: 5.91435  D loss: 0.09444  \n",
      "Train Epoch: [100/200]Step: [100/600] G loss: 5.50685  D loss: 0.04317  \n",
      "Train Epoch: [100/200]Step: [200/600] G loss: 6.16520  D loss: 0.01708  \n",
      "Train Epoch: [100/200]Step: [300/600] G loss: 7.29853  D loss: 0.11542  \n",
      "Train Epoch: [100/200]Step: [400/600] G loss: 5.76509  D loss: 0.22002  \n",
      "Train Epoch: [100/200]Step: [500/600] G loss: 6.69141  D loss: 0.02898  \n",
      "Train Epoch: [101/200]Step: [0/600]   G loss: 5.89873  D loss: 0.07355  \n",
      "Train Epoch: [101/200]Step: [100/600] G loss: 5.53790  D loss: 0.14956  \n",
      "Train Epoch: [101/200]Step: [200/600] G loss: 6.16075  D loss: 0.04380  \n",
      "Train Epoch: [101/200]Step: [300/600] G loss: 6.41946  D loss: 0.07896  \n",
      "Train Epoch: [101/200]Step: [400/600] G loss: 5.60482  D loss: 0.03362  \n",
      "Train Epoch: [101/200]Step: [500/600] G loss: 6.10786  D loss: 0.18064  \n",
      "Train Epoch: [102/200]Step: [0/600]   G loss: 7.81586  D loss: 0.15366  \n",
      "Train Epoch: [102/200]Step: [100/600] G loss: 4.54878  D loss: 0.08736  \n",
      "Train Epoch: [102/200]Step: [200/600] G loss: 5.63682  D loss: 0.17006  \n",
      "Train Epoch: [102/200]Step: [300/600] G loss: 6.43334  D loss: 0.06863  \n",
      "Train Epoch: [102/200]Step: [400/600] G loss: 6.66869  D loss: 0.06568  \n",
      "Train Epoch: [102/200]Step: [500/600] G loss: 6.70319  D loss: 0.26456  \n",
      "Train Epoch: [103/200]Step: [0/600]   G loss: 6.40761  D loss: 0.01912  \n",
      "Train Epoch: [103/200]Step: [100/600] G loss: 6.63235  D loss: 0.10693  \n",
      "Train Epoch: [103/200]Step: [200/600] G loss: 5.92260  D loss: 0.03874  \n",
      "Train Epoch: [103/200]Step: [300/600] G loss: 6.11178  D loss: 0.07266  \n",
      "Train Epoch: [103/200]Step: [400/600] G loss: 6.73291  D loss: 0.03924  \n",
      "Train Epoch: [103/200]Step: [500/600] G loss: 6.60402  D loss: 0.02445  \n",
      "Train Epoch: [104/200]Step: [0/600]   G loss: 7.08944  D loss: 0.06338  \n",
      "Train Epoch: [104/200]Step: [100/600] G loss: 5.68124  D loss: 0.05491  \n",
      "Train Epoch: [104/200]Step: [200/600] G loss: 6.08976  D loss: 0.04298  \n",
      "Train Epoch: [104/200]Step: [300/600] G loss: 6.04673  D loss: 0.02155  \n",
      "Train Epoch: [104/200]Step: [400/600] G loss: 4.79886  D loss: 0.18098  \n",
      "Train Epoch: [104/200]Step: [500/600] G loss: 5.78374  D loss: 0.01995  \n",
      "Train Epoch: [105/200]Step: [0/600]   G loss: 5.02828  D loss: 0.03336  \n",
      "Train Epoch: [105/200]Step: [100/600] G loss: 5.62645  D loss: 0.03416  \n",
      "Train Epoch: [105/200]Step: [200/600] G loss: 6.19035  D loss: 0.02947  \n",
      "Train Epoch: [105/200]Step: [300/600] G loss: 5.77224  D loss: 0.05451  \n",
      "Train Epoch: [105/200]Step: [400/600] G loss: 5.56289  D loss: 0.02967  \n",
      "Train Epoch: [105/200]Step: [500/600] G loss: 5.26924  D loss: 0.13981  \n",
      "Train Epoch: [106/200]Step: [0/600]   G loss: 4.33261  D loss: 0.13062  \n",
      "Train Epoch: [106/200]Step: [100/600] G loss: 5.91608  D loss: 0.03996  \n",
      "Train Epoch: [106/200]Step: [200/600] G loss: 6.40673  D loss: 0.02933  \n",
      "Train Epoch: [106/200]Step: [300/600] G loss: 4.44292  D loss: 0.14770  \n",
      "Train Epoch: [106/200]Step: [400/600] G loss: 6.55211  D loss: 0.07133  \n",
      "Train Epoch: [106/200]Step: [500/600] G loss: 5.61223  D loss: 0.05966  \n",
      "Train Epoch: [107/200]Step: [0/600]   G loss: 5.23891  D loss: 0.05640  \n",
      "Train Epoch: [107/200]Step: [100/600] G loss: 6.61813  D loss: 0.01088  \n",
      "Train Epoch: [107/200]Step: [200/600] G loss: 4.84129  D loss: 0.04314  \n",
      "Train Epoch: [107/200]Step: [300/600] G loss: 4.99194  D loss: 0.07516  \n",
      "Train Epoch: [107/200]Step: [400/600] G loss: 7.06511  D loss: 0.03285  \n",
      "Train Epoch: [107/200]Step: [500/600] G loss: 6.27498  D loss: 0.01022  \n",
      "Train Epoch: [108/200]Step: [0/600]   G loss: 5.47860  D loss: 0.03076  \n",
      "Train Epoch: [108/200]Step: [100/600] G loss: 5.53194  D loss: 0.01507  \n",
      "Train Epoch: [108/200]Step: [200/600] G loss: 6.50705  D loss: 0.07583  \n",
      "Train Epoch: [108/200]Step: [300/600] G loss: 6.60933  D loss: 0.01273  \n",
      "Train Epoch: [108/200]Step: [400/600] G loss: 5.22455  D loss: 0.08460  \n",
      "Train Epoch: [108/200]Step: [500/600] G loss: 5.34110  D loss: 0.09293  \n",
      "Train Epoch: [109/200]Step: [0/600]   G loss: 5.75951  D loss: 0.06639  \n",
      "Train Epoch: [109/200]Step: [100/600] G loss: 4.98194  D loss: 0.13753  \n",
      "Train Epoch: [109/200]Step: [200/600] G loss: 6.69689  D loss: 0.03664  \n",
      "Train Epoch: [109/200]Step: [300/600] G loss: 5.60223  D loss: 0.05511  \n",
      "Train Epoch: [109/200]Step: [400/600] G loss: 5.17165  D loss: 0.04228  \n",
      "Train Epoch: [109/200]Step: [500/600] G loss: 6.79843  D loss: 0.01074  \n",
      "Train Epoch: [110/200]Step: [0/600]   G loss: 7.44179  D loss: 0.01051  \n",
      "Train Epoch: [110/200]Step: [100/600] G loss: 6.42236  D loss: 0.06254  \n",
      "Train Epoch: [110/200]Step: [200/600] G loss: 5.96908  D loss: 0.04452  \n",
      "Train Epoch: [110/200]Step: [300/600] G loss: 5.60158  D loss: 0.02842  \n",
      "Train Epoch: [110/200]Step: [400/600] G loss: 7.20103  D loss: 0.00714  \n",
      "Train Epoch: [110/200]Step: [500/600] G loss: 6.15126  D loss: 0.01877  \n",
      "Train Epoch: [111/200]Step: [0/600]   G loss: 8.35946  D loss: 0.06886  \n",
      "Train Epoch: [111/200]Step: [100/600] G loss: 5.74439  D loss: 0.15157  \n",
      "Train Epoch: [111/200]Step: [200/600] G loss: 4.72084  D loss: 0.07074  \n",
      "Train Epoch: [111/200]Step: [300/600] G loss: 5.58443  D loss: 0.02236  \n",
      "Train Epoch: [111/200]Step: [400/600] G loss: 5.26170  D loss: 0.10279  \n",
      "Train Epoch: [111/200]Step: [500/600] G loss: 6.44026  D loss: 0.10410  \n",
      "Train Epoch: [112/200]Step: [0/600]   G loss: 6.29676  D loss: 0.03268  \n",
      "Train Epoch: [112/200]Step: [100/600] G loss: 7.51336  D loss: 0.26443  \n",
      "Train Epoch: [112/200]Step: [200/600] G loss: 6.94615  D loss: 0.15617  \n",
      "Train Epoch: [112/200]Step: [300/600] G loss: 6.67907  D loss: 0.18769  \n",
      "Train Epoch: [112/200]Step: [400/600] G loss: 6.16420  D loss: 0.03479  \n",
      "Train Epoch: [112/200]Step: [500/600] G loss: 5.31187  D loss: 0.09012  \n",
      "Train Epoch: [113/200]Step: [0/600]   G loss: 7.14082  D loss: 0.12921  \n",
      "Train Epoch: [113/200]Step: [100/600] G loss: 6.25589  D loss: 0.15904  \n",
      "Train Epoch: [113/200]Step: [200/600] G loss: 4.45374  D loss: 0.10978  \n",
      "Train Epoch: [113/200]Step: [300/600] G loss: 4.89418  D loss: 0.06770  \n",
      "Train Epoch: [113/200]Step: [400/600] G loss: 6.07917  D loss: 0.09705  \n",
      "Train Epoch: [113/200]Step: [500/600] G loss: 5.42734  D loss: 0.11971  \n",
      "Train Epoch: [114/200]Step: [0/600]   G loss: 5.10149  D loss: 0.04073  \n",
      "Train Epoch: [114/200]Step: [100/600] G loss: 5.62170  D loss: 0.06365  \n",
      "Train Epoch: [114/200]Step: [200/600] G loss: 5.62567  D loss: 0.21686  \n",
      "Train Epoch: [114/200]Step: [300/600] G loss: 4.27943  D loss: 0.25309  \n",
      "Train Epoch: [114/200]Step: [400/600] G loss: 6.44499  D loss: 0.03762  \n",
      "Train Epoch: [114/200]Step: [500/600] G loss: 5.80233  D loss: 0.04362  \n",
      "Train Epoch: [115/200]Step: [0/600]   G loss: 6.93586  D loss: 0.01799  \n",
      "Train Epoch: [115/200]Step: [100/600] G loss: 5.47457  D loss: 0.12153  \n",
      "Train Epoch: [115/200]Step: [200/600] G loss: 6.07014  D loss: 0.05840  \n",
      "Train Epoch: [115/200]Step: [300/600] G loss: 5.29038  D loss: 0.07670  \n",
      "Train Epoch: [115/200]Step: [400/600] G loss: 6.23423  D loss: 0.02461  \n",
      "Train Epoch: [115/200]Step: [500/600] G loss: 6.03243  D loss: 0.11133  \n",
      "Train Epoch: [116/200]Step: [0/600]   G loss: 7.01225  D loss: 0.28917  \n",
      "Train Epoch: [116/200]Step: [100/600] G loss: 5.84711  D loss: 0.15665  \n",
      "Train Epoch: [116/200]Step: [200/600] G loss: 6.45165  D loss: 0.06398  \n",
      "Train Epoch: [116/200]Step: [300/600] G loss: 5.59815  D loss: 0.09275  \n",
      "Train Epoch: [116/200]Step: [400/600] G loss: 6.38831  D loss: 0.01157  \n",
      "Train Epoch: [116/200]Step: [500/600] G loss: 4.33276  D loss: 0.07091  \n",
      "Train Epoch: [117/200]Step: [0/600]   G loss: 6.19353  D loss: 0.02475  \n",
      "Train Epoch: [117/200]Step: [100/600] G loss: 5.83585  D loss: 0.03124  \n",
      "Train Epoch: [117/200]Step: [200/600] G loss: 7.01697  D loss: 0.11527  \n",
      "Train Epoch: [117/200]Step: [300/600] G loss: 6.07629  D loss: 0.04437  \n",
      "Train Epoch: [117/200]Step: [400/600] G loss: 5.72187  D loss: 0.05756  \n",
      "Train Epoch: [117/200]Step: [500/600] G loss: 5.16937  D loss: 0.07598  \n",
      "Train Epoch: [118/200]Step: [0/600]   G loss: 5.17317  D loss: 0.15705  \n",
      "Train Epoch: [118/200]Step: [100/600] G loss: 5.76122  D loss: 0.04353  \n",
      "Train Epoch: [118/200]Step: [200/600] G loss: 6.61781  D loss: 0.06382  \n",
      "Train Epoch: [118/200]Step: [300/600] G loss: 5.41586  D loss: 0.08817  \n",
      "Train Epoch: [118/200]Step: [400/600] G loss: 6.30049  D loss: 0.01079  \n",
      "Train Epoch: [118/200]Step: [500/600] G loss: 6.58425  D loss: 0.11266  \n",
      "Train Epoch: [119/200]Step: [0/600]   G loss: 5.74991  D loss: 0.06216  \n",
      "Train Epoch: [119/200]Step: [100/600] G loss: 5.18131  D loss: 0.12918  \n",
      "Train Epoch: [119/200]Step: [200/600] G loss: 6.10826  D loss: 0.11010  \n",
      "Train Epoch: [119/200]Step: [300/600] G loss: 5.73091  D loss: 0.08874  \n",
      "Train Epoch: [119/200]Step: [400/600] G loss: 7.17033  D loss: 0.02188  \n",
      "Train Epoch: [119/200]Step: [500/600] G loss: 6.99809  D loss: 0.11139  \n",
      "Train Epoch: [120/200]Step: [0/600]   G loss: 5.38625  D loss: 0.01650  \n",
      "Train Epoch: [120/200]Step: [100/600] G loss: 4.06707  D loss: 0.11761  \n",
      "Train Epoch: [120/200]Step: [200/600] G loss: 5.85151  D loss: 0.01986  \n",
      "Train Epoch: [120/200]Step: [300/600] G loss: 6.03659  D loss: 0.06930  \n",
      "Train Epoch: [120/200]Step: [400/600] G loss: 6.03803  D loss: 0.04700  \n",
      "Train Epoch: [120/200]Step: [500/600] G loss: 6.67122  D loss: 0.05652  \n",
      "Train Epoch: [121/200]Step: [0/600]   G loss: 5.42772  D loss: 0.08034  \n",
      "Train Epoch: [121/200]Step: [100/600] G loss: 6.73976  D loss: 0.02033  \n",
      "Train Epoch: [121/200]Step: [200/600] G loss: 5.55441  D loss: 0.06792  \n",
      "Train Epoch: [121/200]Step: [300/600] G loss: 8.11424  D loss: 0.15334  \n",
      "Train Epoch: [121/200]Step: [400/600] G loss: 6.65014  D loss: 0.03603  \n",
      "Train Epoch: [121/200]Step: [500/600] G loss: 8.45199  D loss: 0.03960  \n",
      "Train Epoch: [122/200]Step: [0/600]   G loss: 6.26650  D loss: 0.03633  \n",
      "Train Epoch: [122/200]Step: [100/600] G loss: 6.31794  D loss: 0.05885  \n",
      "Train Epoch: [122/200]Step: [200/600] G loss: 5.93242  D loss: 0.09098  \n",
      "Train Epoch: [122/200]Step: [300/600] G loss: 6.27751  D loss: 0.13888  \n",
      "Train Epoch: [122/200]Step: [400/600] G loss: 6.47070  D loss: 0.16343  \n",
      "Train Epoch: [122/200]Step: [500/600] G loss: 6.14024  D loss: 0.10143  \n",
      "Train Epoch: [123/200]Step: [0/600]   G loss: 6.79108  D loss: 0.05414  \n",
      "Train Epoch: [123/200]Step: [100/600] G loss: 5.18609  D loss: 0.04636  \n",
      "Train Epoch: [123/200]Step: [200/600] G loss: 5.55556  D loss: 0.05539  \n",
      "Train Epoch: [123/200]Step: [300/600] G loss: 5.78645  D loss: 0.04132  \n",
      "Train Epoch: [123/200]Step: [400/600] G loss: 5.55246  D loss: 0.06820  \n",
      "Train Epoch: [123/200]Step: [500/600] G loss: 5.34982  D loss: 0.14178  \n",
      "Train Epoch: [124/200]Step: [0/600]   G loss: 5.52909  D loss: 0.08718  \n",
      "Train Epoch: [124/200]Step: [100/600] G loss: 5.63054  D loss: 0.18159  \n",
      "Train Epoch: [124/200]Step: [200/600] G loss: 5.36425  D loss: 0.03833  \n",
      "Train Epoch: [124/200]Step: [300/600] G loss: 5.26400  D loss: 0.07706  \n",
      "Train Epoch: [124/200]Step: [400/600] G loss: 4.44999  D loss: 0.16923  \n",
      "Train Epoch: [124/200]Step: [500/600] G loss: 5.71142  D loss: 0.09475  \n",
      "Train Epoch: [125/200]Step: [0/600]   G loss: 5.83559  D loss: 0.02943  \n",
      "Train Epoch: [125/200]Step: [100/600] G loss: 5.68564  D loss: 0.11537  \n",
      "Train Epoch: [125/200]Step: [200/600] G loss: 6.76523  D loss: 0.19272  \n",
      "Train Epoch: [125/200]Step: [300/600] G loss: 5.98075  D loss: 0.10801  \n",
      "Train Epoch: [125/200]Step: [400/600] G loss: 4.91228  D loss: 0.13569  \n",
      "Train Epoch: [125/200]Step: [500/600] G loss: 5.68497  D loss: 0.05413  \n",
      "Train Epoch: [126/200]Step: [0/600]   G loss: 6.41485  D loss: 0.02364  \n",
      "Train Epoch: [126/200]Step: [100/600] G loss: 5.90191  D loss: 0.16591  \n",
      "Train Epoch: [126/200]Step: [200/600] G loss: 5.67269  D loss: 0.08395  \n",
      "Train Epoch: [126/200]Step: [300/600] G loss: 6.64907  D loss: 0.05813  \n",
      "Train Epoch: [126/200]Step: [400/600] G loss: 5.78724  D loss: 0.01859  \n",
      "Train Epoch: [126/200]Step: [500/600] G loss: 6.19071  D loss: 0.11198  \n",
      "Train Epoch: [127/200]Step: [0/600]   G loss: 4.83778  D loss: 0.11303  \n",
      "Train Epoch: [127/200]Step: [100/600] G loss: 5.92654  D loss: 0.04757  \n",
      "Train Epoch: [127/200]Step: [200/600] G loss: 5.03331  D loss: 0.16772  \n",
      "Train Epoch: [127/200]Step: [300/600] G loss: 5.12247  D loss: 0.05979  \n",
      "Train Epoch: [127/200]Step: [400/600] G loss: 5.11578  D loss: 0.02628  \n",
      "Train Epoch: [127/200]Step: [500/600] G loss: 5.01478  D loss: 0.01267  \n",
      "Train Epoch: [128/200]Step: [0/600]   G loss: 4.92546  D loss: 0.06337  \n",
      "Train Epoch: [128/200]Step: [100/600] G loss: 6.11508  D loss: 0.05523  \n",
      "Train Epoch: [128/200]Step: [200/600] G loss: 4.73418  D loss: 0.10072  \n",
      "Train Epoch: [128/200]Step: [300/600] G loss: 5.89611  D loss: 0.11652  \n",
      "Train Epoch: [128/200]Step: [400/600] G loss: 6.21050  D loss: 0.09169  \n",
      "Train Epoch: [128/200]Step: [500/600] G loss: 6.08554  D loss: 0.09856  \n",
      "Train Epoch: [129/200]Step: [0/600]   G loss: 5.34473  D loss: 0.10255  \n",
      "Train Epoch: [129/200]Step: [100/600] G loss: 5.40364  D loss: 0.03524  \n",
      "Train Epoch: [129/200]Step: [200/600] G loss: 6.13789  D loss: 0.05249  \n",
      "Train Epoch: [129/200]Step: [300/600] G loss: 5.41870  D loss: 0.02908  \n",
      "Train Epoch: [129/200]Step: [400/600] G loss: 6.07207  D loss: 0.04907  \n",
      "Train Epoch: [129/200]Step: [500/600] G loss: 5.15219  D loss: 0.08520  \n",
      "Train Epoch: [130/200]Step: [0/600]   G loss: 7.41306  D loss: 0.06572  \n",
      "Train Epoch: [130/200]Step: [100/600] G loss: 5.21037  D loss: 0.04890  \n",
      "Train Epoch: [130/200]Step: [200/600] G loss: 5.26639  D loss: 0.04878  \n",
      "Train Epoch: [130/200]Step: [300/600] G loss: 4.88263  D loss: 0.11087  \n",
      "Train Epoch: [130/200]Step: [400/600] G loss: 5.91424  D loss: 0.03068  \n",
      "Train Epoch: [130/200]Step: [500/600] G loss: 6.12280  D loss: 0.04708  \n",
      "Train Epoch: [131/200]Step: [0/600]   G loss: 6.49577  D loss: 0.08265  \n",
      "Train Epoch: [131/200]Step: [100/600] G loss: 7.01398  D loss: 0.02977  \n",
      "Train Epoch: [131/200]Step: [200/600] G loss: 5.19031  D loss: 0.04072  \n",
      "Train Epoch: [131/200]Step: [300/600] G loss: 5.88926  D loss: 0.17280  \n",
      "Train Epoch: [131/200]Step: [400/600] G loss: 8.13386  D loss: 0.03260  \n",
      "Train Epoch: [131/200]Step: [500/600] G loss: 8.04929  D loss: 0.03328  \n",
      "Train Epoch: [132/200]Step: [0/600]   G loss: 6.23599  D loss: 0.06900  \n",
      "Train Epoch: [132/200]Step: [100/600] G loss: 7.78416  D loss: 0.09593  \n",
      "Train Epoch: [132/200]Step: [200/600] G loss: 4.57334  D loss: 0.10817  \n",
      "Train Epoch: [132/200]Step: [300/600] G loss: 5.22291  D loss: 0.04100  \n",
      "Train Epoch: [132/200]Step: [400/600] G loss: 6.94610  D loss: 0.00850  \n",
      "Train Epoch: [132/200]Step: [500/600] G loss: 5.31478  D loss: 0.03735  \n",
      "Train Epoch: [133/200]Step: [0/600]   G loss: 6.70602  D loss: 0.10756  \n",
      "Train Epoch: [133/200]Step: [100/600] G loss: 4.41523  D loss: 0.18397  \n",
      "Train Epoch: [133/200]Step: [200/600] G loss: 5.27918  D loss: 0.12794  \n",
      "Train Epoch: [133/200]Step: [300/600] G loss: 4.67646  D loss: 0.04146  \n",
      "Train Epoch: [133/200]Step: [400/600] G loss: 4.94346  D loss: 0.17512  \n",
      "Train Epoch: [133/200]Step: [500/600] G loss: 3.79163  D loss: 0.19562  \n",
      "Train Epoch: [134/200]Step: [0/600]   G loss: 5.24765  D loss: 0.03856  \n",
      "Train Epoch: [134/200]Step: [100/600] G loss: 6.27119  D loss: 0.06754  \n",
      "Train Epoch: [134/200]Step: [200/600] G loss: 6.72278  D loss: 0.06053  \n",
      "Train Epoch: [134/200]Step: [300/600] G loss: 5.38236  D loss: 0.02771  \n",
      "Train Epoch: [134/200]Step: [400/600] G loss: 6.32253  D loss: 0.01700  \n",
      "Train Epoch: [134/200]Step: [500/600] G loss: 5.16265  D loss: 0.07377  \n",
      "Train Epoch: [135/200]Step: [0/600]   G loss: 5.38704  D loss: 0.03924  \n",
      "Train Epoch: [135/200]Step: [100/600] G loss: 4.69991  D loss: 0.06528  \n",
      "Train Epoch: [135/200]Step: [200/600] G loss: 6.10148  D loss: 0.05690  \n",
      "Train Epoch: [135/200]Step: [300/600] G loss: 5.63398  D loss: 0.05566  \n",
      "Train Epoch: [135/200]Step: [400/600] G loss: 6.93116  D loss: 0.08844  \n",
      "Train Epoch: [135/200]Step: [500/600] G loss: 7.11406  D loss: 0.14394  \n",
      "Train Epoch: [136/200]Step: [0/600]   G loss: 5.93506  D loss: 0.15552  \n",
      "Train Epoch: [136/200]Step: [100/600] G loss: 6.49220  D loss: 0.12992  \n",
      "Train Epoch: [136/200]Step: [200/600] G loss: 5.89965  D loss: 0.02851  \n",
      "Train Epoch: [136/200]Step: [300/600] G loss: 7.23892  D loss: 0.16898  \n",
      "Train Epoch: [136/200]Step: [400/600] G loss: 6.50985  D loss: 0.09024  \n",
      "Train Epoch: [136/200]Step: [500/600] G loss: 8.27082  D loss: 0.08749  \n",
      "Train Epoch: [137/200]Step: [0/600]   G loss: 5.25829  D loss: 0.07916  \n",
      "Train Epoch: [137/200]Step: [100/600] G loss: 8.33108  D loss: 0.10746  \n",
      "Train Epoch: [137/200]Step: [200/600] G loss: 6.83427  D loss: 0.10511  \n",
      "Train Epoch: [137/200]Step: [300/600] G loss: 5.63459  D loss: 0.07294  \n",
      "Train Epoch: [137/200]Step: [400/600] G loss: 5.54234  D loss: 0.06848  \n",
      "Train Epoch: [137/200]Step: [500/600] G loss: 6.13561  D loss: 0.01323  \n",
      "Train Epoch: [138/200]Step: [0/600]   G loss: 5.00873  D loss: 0.07291  \n",
      "Train Epoch: [138/200]Step: [100/600] G loss: 5.79504  D loss: 0.07867  \n",
      "Train Epoch: [138/200]Step: [200/600] G loss: 6.35512  D loss: 0.18110  \n",
      "Train Epoch: [138/200]Step: [300/600] G loss: 6.94455  D loss: 0.02691  \n",
      "Train Epoch: [138/200]Step: [400/600] G loss: 4.16174  D loss: 0.15230  \n",
      "Train Epoch: [138/200]Step: [500/600] G loss: 5.46403  D loss: 0.12117  \n",
      "Train Epoch: [139/200]Step: [0/600]   G loss: 7.96741  D loss: 0.00453  \n",
      "Train Epoch: [139/200]Step: [100/600] G loss: 6.33793  D loss: 0.07096  \n",
      "Train Epoch: [139/200]Step: [200/600] G loss: 5.73448  D loss: 0.09060  \n",
      "Train Epoch: [139/200]Step: [300/600] G loss: 5.54526  D loss: 0.08294  \n",
      "Train Epoch: [139/200]Step: [400/600] G loss: 6.70470  D loss: 0.07124  \n",
      "Train Epoch: [139/200]Step: [500/600] G loss: 4.74308  D loss: 0.08766  \n",
      "Train Epoch: [140/200]Step: [0/600]   G loss: 5.69103  D loss: 0.11075  \n",
      "Train Epoch: [140/200]Step: [100/600] G loss: 5.68581  D loss: 0.01928  \n",
      "Train Epoch: [140/200]Step: [200/600] G loss: 5.40043  D loss: 0.05856  \n",
      "Train Epoch: [140/200]Step: [300/600] G loss: 5.62465  D loss: 0.04638  \n",
      "Train Epoch: [140/200]Step: [400/600] G loss: 5.89217  D loss: 0.06323  \n",
      "Train Epoch: [140/200]Step: [500/600] G loss: 7.65718  D loss: 0.03722  \n",
      "Train Epoch: [141/200]Step: [0/600]   G loss: 5.81038  D loss: 0.14167  \n",
      "Train Epoch: [141/200]Step: [100/600] G loss: 5.10841  D loss: 0.05345  \n",
      "Train Epoch: [141/200]Step: [200/600] G loss: 7.29506  D loss: 0.01661  \n",
      "Train Epoch: [141/200]Step: [300/600] G loss: 6.24738  D loss: 0.02254  \n",
      "Train Epoch: [141/200]Step: [400/600] G loss: 6.14568  D loss: 0.07848  \n",
      "Train Epoch: [141/200]Step: [500/600] G loss: 6.68878  D loss: 0.06618  \n",
      "Train Epoch: [142/200]Step: [0/600]   G loss: 7.50446  D loss: 0.00926  \n",
      "Train Epoch: [142/200]Step: [100/600] G loss: 5.30536  D loss: 0.06922  \n",
      "Train Epoch: [142/200]Step: [200/600] G loss: 6.91387  D loss: 0.02677  \n",
      "Train Epoch: [142/200]Step: [300/600] G loss: 5.78253  D loss: 0.09654  \n",
      "Train Epoch: [142/200]Step: [400/600] G loss: 6.85691  D loss: 0.01396  \n",
      "Train Epoch: [142/200]Step: [500/600] G loss: 6.97741  D loss: 0.16200  \n",
      "Train Epoch: [143/200]Step: [0/600]   G loss: 6.67569  D loss: 0.09817  \n",
      "Train Epoch: [143/200]Step: [100/600] G loss: 6.41997  D loss: 0.03287  \n",
      "Train Epoch: [143/200]Step: [200/600] G loss: 5.14125  D loss: 0.03384  \n",
      "Train Epoch: [143/200]Step: [300/600] G loss: 6.11770  D loss: 0.02862  \n",
      "Train Epoch: [143/200]Step: [400/600] G loss: 6.30109  D loss: 0.07517  \n",
      "Train Epoch: [143/200]Step: [500/600] G loss: 5.14662  D loss: 0.01261  \n",
      "Train Epoch: [144/200]Step: [0/600]   G loss: 4.87299  D loss: 0.02018  \n",
      "Train Epoch: [144/200]Step: [100/600] G loss: 4.79561  D loss: 0.05184  \n",
      "Train Epoch: [144/200]Step: [200/600] G loss: 5.46434  D loss: 0.01258  \n",
      "Train Epoch: [144/200]Step: [300/600] G loss: 6.19870  D loss: 0.03654  \n",
      "Train Epoch: [144/200]Step: [400/600] G loss: 6.47451  D loss: 0.11189  \n",
      "Train Epoch: [144/200]Step: [500/600] G loss: 5.45619  D loss: 0.01089  \n",
      "Train Epoch: [145/200]Step: [0/600]   G loss: 5.86476  D loss: 0.03857  \n",
      "Train Epoch: [145/200]Step: [100/600] G loss: 6.11857  D loss: 0.03366  \n",
      "Train Epoch: [145/200]Step: [200/600] G loss: 7.08146  D loss: 0.01262  \n",
      "Train Epoch: [145/200]Step: [300/600] G loss: 4.93544  D loss: 0.02582  \n",
      "Train Epoch: [145/200]Step: [400/600] G loss: 6.99301  D loss: 0.00702  \n",
      "Train Epoch: [145/200]Step: [500/600] G loss: 7.04953  D loss: 0.12384  \n",
      "Train Epoch: [146/200]Step: [0/600]   G loss: 7.06002  D loss: 0.01626  \n",
      "Train Epoch: [146/200]Step: [100/600] G loss: 7.07133  D loss: 0.01989  \n",
      "Train Epoch: [146/200]Step: [200/600] G loss: 6.35527  D loss: 0.05653  \n",
      "Train Epoch: [146/200]Step: [300/600] G loss: 5.95442  D loss: 0.10631  \n",
      "Train Epoch: [146/200]Step: [400/600] G loss: 5.97789  D loss: 0.08840  \n",
      "Train Epoch: [146/200]Step: [500/600] G loss: 5.76725  D loss: 0.02881  \n",
      "Train Epoch: [147/200]Step: [0/600]   G loss: 6.86180  D loss: 0.01334  \n",
      "Train Epoch: [147/200]Step: [100/600] G loss: 6.75171  D loss: 0.02116  \n",
      "Train Epoch: [147/200]Step: [200/600] G loss: 5.75106  D loss: 0.03126  \n",
      "Train Epoch: [147/200]Step: [300/600] G loss: 5.35971  D loss: 0.14132  \n",
      "Train Epoch: [147/200]Step: [400/600] G loss: 6.49535  D loss: 0.09652  \n",
      "Train Epoch: [147/200]Step: [500/600] G loss: 5.31098  D loss: 0.03362  \n",
      "Train Epoch: [148/200]Step: [0/600]   G loss: 5.59144  D loss: 0.01110  \n",
      "Train Epoch: [148/200]Step: [100/600] G loss: 6.31520  D loss: 0.08052  \n",
      "Train Epoch: [148/200]Step: [200/600] G loss: 5.81377  D loss: 0.00570  \n",
      "Train Epoch: [148/200]Step: [300/600] G loss: 6.99952  D loss: 0.00565  \n",
      "Train Epoch: [148/200]Step: [400/600] G loss: 6.42220  D loss: 0.02933  \n",
      "Train Epoch: [148/200]Step: [500/600] G loss: 8.38530  D loss: 0.10795  \n",
      "Train Epoch: [149/200]Step: [0/600]   G loss: 6.35728  D loss: 0.01308  \n",
      "Train Epoch: [149/200]Step: [100/600] G loss: 6.99568  D loss: 0.01558  \n",
      "Train Epoch: [149/200]Step: [200/600] G loss: 7.28856  D loss: 0.03476  \n",
      "Train Epoch: [149/200]Step: [300/600] G loss: 6.21340  D loss: 0.01935  \n",
      "Train Epoch: [149/200]Step: [400/600] G loss: 6.75777  D loss: 0.01839  \n",
      "Train Epoch: [149/200]Step: [500/600] G loss: 4.86682  D loss: 0.03128  \n",
      "Train Epoch: [150/200]Step: [0/600]   G loss: 7.53062  D loss: 0.01838  \n",
      "Train Epoch: [150/200]Step: [100/600] G loss: 6.24378  D loss: 0.05857  \n",
      "Train Epoch: [150/200]Step: [200/600] G loss: 7.17969  D loss: 0.00700  \n",
      "Train Epoch: [150/200]Step: [300/600] G loss: 6.67337  D loss: 0.01172  \n",
      "Train Epoch: [150/200]Step: [400/600] G loss: 6.38051  D loss: 0.08415  \n",
      "Train Epoch: [150/200]Step: [500/600] G loss: 6.66680  D loss: 0.14230  \n",
      "Train Epoch: [151/200]Step: [0/600]   G loss: 4.87634  D loss: 0.02580  \n",
      "Train Epoch: [151/200]Step: [100/600] G loss: 6.35365  D loss: 0.14213  \n",
      "Train Epoch: [151/200]Step: [200/600] G loss: 7.63332  D loss: 0.03769  \n",
      "Train Epoch: [151/200]Step: [300/600] G loss: 6.47133  D loss: 0.06462  \n",
      "Train Epoch: [151/200]Step: [400/600] G loss: 7.02670  D loss: 0.01546  \n",
      "Train Epoch: [151/200]Step: [500/600] G loss: 7.38070  D loss: 0.04966  \n",
      "Train Epoch: [152/200]Step: [0/600]   G loss: 6.59367  D loss: 0.03461  \n",
      "Train Epoch: [152/200]Step: [100/600] G loss: 6.28312  D loss: 0.00619  \n",
      "Train Epoch: [152/200]Step: [200/600] G loss: 8.68490  D loss: 0.05273  \n",
      "Train Epoch: [152/200]Step: [300/600] G loss: 5.61365  D loss: 0.08961  \n",
      "Train Epoch: [152/200]Step: [400/600] G loss: 8.27822  D loss: 0.03537  \n",
      "Train Epoch: [152/200]Step: [500/600] G loss: 7.65590  D loss: 0.01354  \n",
      "Train Epoch: [153/200]Step: [0/600]   G loss: 5.96262  D loss: 0.02822  \n",
      "Train Epoch: [153/200]Step: [100/600] G loss: 4.92467  D loss: 0.09349  \n",
      "Train Epoch: [153/200]Step: [200/600] G loss: 7.89406  D loss: 0.03850  \n",
      "Train Epoch: [153/200]Step: [300/600] G loss: 6.58061  D loss: 0.06696  \n",
      "Train Epoch: [153/200]Step: [400/600] G loss: 7.81309  D loss: 0.01477  \n",
      "Train Epoch: [153/200]Step: [500/600] G loss: 5.89671  D loss: 0.26763  \n",
      "Train Epoch: [154/200]Step: [0/600]   G loss: 6.78504  D loss: 0.09861  \n",
      "Train Epoch: [154/200]Step: [100/600] G loss: 5.87008  D loss: 0.01964  \n",
      "Train Epoch: [154/200]Step: [200/600] G loss: 8.89266  D loss: 0.04309  \n",
      "Train Epoch: [154/200]Step: [300/600] G loss: 6.05051  D loss: 0.03865  \n",
      "Train Epoch: [154/200]Step: [400/600] G loss: 7.93578  D loss: 0.10861  \n",
      "Train Epoch: [154/200]Step: [500/600] G loss: 6.51962  D loss: 0.03626  \n",
      "Train Epoch: [155/200]Step: [0/600]   G loss: 5.76730  D loss: 0.01271  \n",
      "Train Epoch: [155/200]Step: [100/600] G loss: 4.71820  D loss: 0.12949  \n",
      "Train Epoch: [155/200]Step: [200/600] G loss: 4.64106  D loss: 0.16327  \n",
      "Train Epoch: [155/200]Step: [300/600] G loss: 6.65789  D loss: 0.02330  \n",
      "Train Epoch: [155/200]Step: [400/600] G loss: 7.17099  D loss: 0.14354  \n",
      "Train Epoch: [155/200]Step: [500/600] G loss: 5.19572  D loss: 0.11482  \n",
      "Train Epoch: [156/200]Step: [0/600]   G loss: 7.49143  D loss: 0.07870  \n",
      "Train Epoch: [156/200]Step: [100/600] G loss: 5.87210  D loss: 0.17028  \n",
      "Train Epoch: [156/200]Step: [200/600] G loss: 6.36023  D loss: 0.18481  \n",
      "Train Epoch: [156/200]Step: [300/600] G loss: 7.87004  D loss: 0.05611  \n",
      "Train Epoch: [156/200]Step: [400/600] G loss: 7.57563  D loss: 0.00377  \n",
      "Train Epoch: [156/200]Step: [500/600] G loss: 7.26945  D loss: 0.10688  \n",
      "Train Epoch: [157/200]Step: [0/600]   G loss: 7.43195  D loss: 0.00669  \n",
      "Train Epoch: [157/200]Step: [100/600] G loss: 5.16374  D loss: 0.12572  \n",
      "Train Epoch: [157/200]Step: [200/600] G loss: 5.81310  D loss: 0.01126  \n",
      "Train Epoch: [157/200]Step: [300/600] G loss: 5.23573  D loss: 0.12863  \n",
      "Train Epoch: [157/200]Step: [400/600] G loss: 6.12659  D loss: 0.00862  \n",
      "Train Epoch: [157/200]Step: [500/600] G loss: 5.85365  D loss: 0.04945  \n",
      "Train Epoch: [158/200]Step: [0/600]   G loss: 6.47747  D loss: 0.02337  \n",
      "Train Epoch: [158/200]Step: [100/600] G loss: 7.66010  D loss: 0.06935  \n",
      "Train Epoch: [158/200]Step: [200/600] G loss: 5.55435  D loss: 0.03745  \n",
      "Train Epoch: [158/200]Step: [300/600] G loss: 6.18428  D loss: 0.05867  \n",
      "Train Epoch: [158/200]Step: [400/600] G loss: 6.54164  D loss: 0.03002  \n",
      "Train Epoch: [158/200]Step: [500/600] G loss: 6.66890  D loss: 0.11703  \n",
      "Train Epoch: [159/200]Step: [0/600]   G loss: 5.75576  D loss: 0.04281  \n",
      "Train Epoch: [159/200]Step: [100/600] G loss: 4.97694  D loss: 0.01416  \n",
      "Train Epoch: [159/200]Step: [200/600] G loss: 7.38517  D loss: 0.05942  \n",
      "Train Epoch: [159/200]Step: [300/600] G loss: 6.30991  D loss: 0.02325  \n",
      "Train Epoch: [159/200]Step: [400/600] G loss: 4.62678  D loss: 0.03443  \n",
      "Train Epoch: [159/200]Step: [500/600] G loss: 7.53500  D loss: 0.07317  \n",
      "Train Epoch: [160/200]Step: [0/600]   G loss: 4.87483  D loss: 0.05761  \n",
      "Train Epoch: [160/200]Step: [100/600] G loss: 6.93161  D loss: 0.06166  \n",
      "Train Epoch: [160/200]Step: [200/600] G loss: 6.13118  D loss: 0.00928  \n",
      "Train Epoch: [160/200]Step: [300/600] G loss: 6.37340  D loss: 0.07891  \n",
      "Train Epoch: [160/200]Step: [400/600] G loss: 6.13662  D loss: 0.09688  \n",
      "Train Epoch: [160/200]Step: [500/600] G loss: 4.28868  D loss: 0.07802  \n",
      "Train Epoch: [161/200]Step: [0/600]   G loss: 4.79226  D loss: 0.07402  \n",
      "Train Epoch: [161/200]Step: [100/600] G loss: 6.65884  D loss: 0.01705  \n",
      "Train Epoch: [161/200]Step: [200/600] G loss: 6.48918  D loss: 0.01502  \n",
      "Train Epoch: [161/200]Step: [300/600] G loss: 7.77728  D loss: 0.00822  \n",
      "Train Epoch: [161/200]Step: [400/600] G loss: 6.34823  D loss: 0.02740  \n",
      "Train Epoch: [161/200]Step: [500/600] G loss: 6.09207  D loss: 0.02124  \n",
      "Train Epoch: [162/200]Step: [0/600]   G loss: 6.04474  D loss: 0.00990  \n",
      "Train Epoch: [162/200]Step: [100/600] G loss: 5.91266  D loss: 0.01928  \n",
      "Train Epoch: [162/200]Step: [200/600] G loss: 6.45857  D loss: 0.10412  \n",
      "Train Epoch: [162/200]Step: [300/600] G loss: 6.89602  D loss: 0.08743  \n",
      "Train Epoch: [162/200]Step: [400/600] G loss: 7.15098  D loss: 0.06358  \n",
      "Train Epoch: [162/200]Step: [500/600] G loss: 6.69112  D loss: 0.13113  \n",
      "Train Epoch: [163/200]Step: [0/600]   G loss: 5.84477  D loss: 0.13420  \n",
      "Train Epoch: [163/200]Step: [100/600] G loss: 7.91789  D loss: 0.01433  \n",
      "Train Epoch: [163/200]Step: [200/600] G loss: 6.38093  D loss: 0.12633  \n",
      "Train Epoch: [163/200]Step: [300/600] G loss: 5.85794  D loss: 0.07929  \n",
      "Train Epoch: [163/200]Step: [400/600] G loss: 7.12885  D loss: 0.18729  \n",
      "Train Epoch: [163/200]Step: [500/600] G loss: 6.12789  D loss: 0.19452  \n",
      "Train Epoch: [164/200]Step: [0/600]   G loss: 6.27020  D loss: 0.01436  \n",
      "Train Epoch: [164/200]Step: [100/600] G loss: 6.25659  D loss: 0.13956  \n",
      "Train Epoch: [164/200]Step: [200/600] G loss: 4.37369  D loss: 0.08634  \n",
      "Train Epoch: [164/200]Step: [300/600] G loss: 5.21842  D loss: 0.07724  \n",
      "Train Epoch: [164/200]Step: [400/600] G loss: 7.36795  D loss: 0.01225  \n",
      "Train Epoch: [164/200]Step: [500/600] G loss: 5.60543  D loss: 0.02237  \n",
      "Train Epoch: [165/200]Step: [0/600]   G loss: 5.96671  D loss: 0.06864  \n",
      "Train Epoch: [165/200]Step: [100/600] G loss: 5.73160  D loss: 0.02428  \n",
      "Train Epoch: [165/200]Step: [200/600] G loss: 5.42682  D loss: 0.04907  \n",
      "Train Epoch: [165/200]Step: [300/600] G loss: 5.63189  D loss: 0.06407  \n",
      "Train Epoch: [165/200]Step: [400/600] G loss: 7.27894  D loss: 0.03670  \n",
      "Train Epoch: [165/200]Step: [500/600] G loss: 5.62916  D loss: 0.01049  \n",
      "Train Epoch: [166/200]Step: [0/600]   G loss: 7.81067  D loss: 0.04443  \n",
      "Train Epoch: [166/200]Step: [100/600] G loss: 6.08020  D loss: 0.08595  \n",
      "Train Epoch: [166/200]Step: [200/600] G loss: 6.42810  D loss: 0.05448  \n",
      "Train Epoch: [166/200]Step: [300/600] G loss: 5.84120  D loss: 0.10931  \n",
      "Train Epoch: [166/200]Step: [400/600] G loss: 5.85171  D loss: 0.03828  \n",
      "Train Epoch: [166/200]Step: [500/600] G loss: 7.25543  D loss: 0.15516  \n",
      "Train Epoch: [167/200]Step: [0/600]   G loss: 5.39732  D loss: 0.11649  \n",
      "Train Epoch: [167/200]Step: [100/600] G loss: 5.06315  D loss: 0.12595  \n",
      "Train Epoch: [167/200]Step: [200/600] G loss: 5.32204  D loss: 0.06093  \n",
      "Train Epoch: [167/200]Step: [300/600] G loss: 6.10441  D loss: 0.02179  \n",
      "Train Epoch: [167/200]Step: [400/600] G loss: 6.07206  D loss: 0.07163  \n",
      "Train Epoch: [167/200]Step: [500/600] G loss: 5.56679  D loss: 0.05402  \n",
      "Train Epoch: [168/200]Step: [0/600]   G loss: 4.81372  D loss: 0.08766  \n",
      "Train Epoch: [168/200]Step: [100/600] G loss: 5.75993  D loss: 0.08924  \n",
      "Train Epoch: [168/200]Step: [200/600] G loss: 6.49512  D loss: 0.12530  \n",
      "Train Epoch: [168/200]Step: [300/600] G loss: 5.05489  D loss: 0.09709  \n",
      "Train Epoch: [168/200]Step: [400/600] G loss: 6.19031  D loss: 0.05221  \n",
      "Train Epoch: [168/200]Step: [500/600] G loss: 5.52650  D loss: 0.12686  \n",
      "Train Epoch: [169/200]Step: [0/600]   G loss: 5.89479  D loss: 0.03458  \n",
      "Train Epoch: [169/200]Step: [100/600] G loss: 5.22988  D loss: 0.10854  \n",
      "Train Epoch: [169/200]Step: [200/600] G loss: 7.09360  D loss: 0.01561  \n",
      "Train Epoch: [169/200]Step: [300/600] G loss: 8.71116  D loss: 0.01925  \n",
      "Train Epoch: [169/200]Step: [400/600] G loss: 5.80300  D loss: 0.02722  \n",
      "Train Epoch: [169/200]Step: [500/600] G loss: 7.85072  D loss: 0.05561  \n",
      "Train Epoch: [170/200]Step: [0/600]   G loss: 6.61219  D loss: 0.15129  \n",
      "Train Epoch: [170/200]Step: [100/600] G loss: 5.37925  D loss: 0.13217  \n",
      "Train Epoch: [170/200]Step: [200/600] G loss: 5.83139  D loss: 0.11477  \n",
      "Train Epoch: [170/200]Step: [300/600] G loss: 5.61417  D loss: 0.02684  \n",
      "Train Epoch: [170/200]Step: [400/600] G loss: 6.75241  D loss: 0.03018  \n",
      "Train Epoch: [170/200]Step: [500/600] G loss: 6.46365  D loss: 0.02969  \n",
      "Train Epoch: [171/200]Step: [0/600]   G loss: 5.99449  D loss: 0.03600  \n",
      "Train Epoch: [171/200]Step: [100/600] G loss: 7.27750  D loss: 0.19073  \n",
      "Train Epoch: [171/200]Step: [200/600] G loss: 6.83095  D loss: 0.04633  \n",
      "Train Epoch: [171/200]Step: [300/600] G loss: 5.92656  D loss: 0.05781  \n",
      "Train Epoch: [171/200]Step: [400/600] G loss: 5.98215  D loss: 0.07415  \n",
      "Train Epoch: [171/200]Step: [500/600] G loss: 7.22131  D loss: 0.16565  \n",
      "Train Epoch: [172/200]Step: [0/600]   G loss: 4.94407  D loss: 0.08623  \n",
      "Train Epoch: [172/200]Step: [100/600] G loss: 5.89763  D loss: 0.09053  \n",
      "Train Epoch: [172/200]Step: [200/600] G loss: 7.25266  D loss: 0.04004  \n",
      "Train Epoch: [172/200]Step: [300/600] G loss: 5.47837  D loss: 0.03354  \n",
      "Train Epoch: [172/200]Step: [400/600] G loss: 5.31320  D loss: 0.12334  \n",
      "Train Epoch: [172/200]Step: [500/600] G loss: 7.02999  D loss: 0.00985  \n",
      "Train Epoch: [173/200]Step: [0/600]   G loss: 5.90611  D loss: 0.03764  \n",
      "Train Epoch: [173/200]Step: [100/600] G loss: 7.01968  D loss: 0.06450  \n",
      "Train Epoch: [173/200]Step: [200/600] G loss: 5.51024  D loss: 0.03930  \n",
      "Train Epoch: [173/200]Step: [300/600] G loss: 5.23190  D loss: 0.04025  \n",
      "Train Epoch: [173/200]Step: [400/600] G loss: 4.92040  D loss: 0.06099  \n",
      "Train Epoch: [173/200]Step: [500/600] G loss: 6.00924  D loss: 0.02574  \n",
      "Train Epoch: [174/200]Step: [0/600]   G loss: 6.04443  D loss: 0.06616  \n",
      "Train Epoch: [174/200]Step: [100/600] G loss: 7.04233  D loss: 0.05509  \n",
      "Train Epoch: [174/200]Step: [200/600] G loss: 7.67607  D loss: 0.05921  \n",
      "Train Epoch: [174/200]Step: [300/600] G loss: 5.72601  D loss: 0.12247  \n",
      "Train Epoch: [174/200]Step: [400/600] G loss: 5.93828  D loss: 0.06852  \n",
      "Train Epoch: [174/200]Step: [500/600] G loss: 5.35010  D loss: 0.03589  \n",
      "Train Epoch: [175/200]Step: [0/600]   G loss: 6.03058  D loss: 0.04430  \n",
      "Train Epoch: [175/200]Step: [100/600] G loss: 6.03681  D loss: 0.03907  \n",
      "Train Epoch: [175/200]Step: [200/600] G loss: 5.99821  D loss: 0.01710  \n",
      "Train Epoch: [175/200]Step: [300/600] G loss: 6.18130  D loss: 0.01647  \n",
      "Train Epoch: [175/200]Step: [400/600] G loss: 6.28416  D loss: 0.07858  \n",
      "Train Epoch: [175/200]Step: [500/600] G loss: 8.11683  D loss: 0.06038  \n",
      "Train Epoch: [176/200]Step: [0/600]   G loss: 5.35859  D loss: 0.05787  \n",
      "Train Epoch: [176/200]Step: [100/600] G loss: 6.36179  D loss: 0.03204  \n",
      "Train Epoch: [176/200]Step: [200/600] G loss: 6.44401  D loss: 0.02391  \n",
      "Train Epoch: [176/200]Step: [300/600] G loss: 6.19848  D loss: 0.00791  \n",
      "Train Epoch: [176/200]Step: [400/600] G loss: 6.77358  D loss: 0.01923  \n",
      "Train Epoch: [176/200]Step: [500/600] G loss: 6.62848  D loss: 0.02393  \n",
      "Train Epoch: [177/200]Step: [0/600]   G loss: 6.90805  D loss: 0.08649  \n",
      "Train Epoch: [177/200]Step: [100/600] G loss: 5.13016  D loss: 0.02633  \n",
      "Train Epoch: [177/200]Step: [200/600] G loss: 7.52429  D loss: 0.14074  \n",
      "Train Epoch: [177/200]Step: [300/600] G loss: 8.72343  D loss: 0.03425  \n",
      "Train Epoch: [177/200]Step: [400/600] G loss: 5.31822  D loss: 0.13619  \n",
      "Train Epoch: [177/200]Step: [500/600] G loss: 6.31504  D loss: 0.01057  \n",
      "Train Epoch: [178/200]Step: [0/600]   G loss: 6.96710  D loss: 0.02454  \n",
      "Train Epoch: [178/200]Step: [100/600] G loss: 7.04604  D loss: 0.02204  \n",
      "Train Epoch: [178/200]Step: [200/600] G loss: 6.24083  D loss: 0.09229  \n",
      "Train Epoch: [178/200]Step: [300/600] G loss: 7.04128  D loss: 0.03141  \n",
      "Train Epoch: [178/200]Step: [400/600] G loss: 5.90018  D loss: 0.01019  \n",
      "Train Epoch: [178/200]Step: [500/600] G loss: 6.25651  D loss: 0.02335  \n",
      "Train Epoch: [179/200]Step: [0/600]   G loss: 5.37615  D loss: 0.07799  \n",
      "Train Epoch: [179/200]Step: [100/600] G loss: 7.60793  D loss: 0.02811  \n",
      "Train Epoch: [179/200]Step: [200/600] G loss: 6.42848  D loss: 0.03504  \n",
      "Train Epoch: [179/200]Step: [300/600] G loss: 6.26544  D loss: 0.11627  \n",
      "Train Epoch: [179/200]Step: [400/600] G loss: 8.43270  D loss: 0.10013  \n",
      "Train Epoch: [179/200]Step: [500/600] G loss: 6.27881  D loss: 0.10158  \n",
      "Train Epoch: [180/200]Step: [0/600]   G loss: 6.70383  D loss: 0.12069  \n",
      "Train Epoch: [180/200]Step: [100/600] G loss: 7.42903  D loss: 0.16884  \n",
      "Train Epoch: [180/200]Step: [200/600] G loss: 6.87166  D loss: 0.06415  \n",
      "Train Epoch: [180/200]Step: [300/600] G loss: 5.43817  D loss: 0.03684  \n",
      "Train Epoch: [180/200]Step: [400/600] G loss: 5.56606  D loss: 0.10447  \n",
      "Train Epoch: [180/200]Step: [500/600] G loss: 5.98797  D loss: 0.02542  \n",
      "Train Epoch: [181/200]Step: [0/600]   G loss: 5.33995  D loss: 0.11089  \n",
      "Train Epoch: [181/200]Step: [100/600] G loss: 7.46038  D loss: 0.04079  \n",
      "Train Epoch: [181/200]Step: [200/600] G loss: 6.24371  D loss: 0.15017  \n",
      "Train Epoch: [181/200]Step: [300/600] G loss: 6.00499  D loss: 0.11432  \n",
      "Train Epoch: [181/200]Step: [400/600] G loss: 6.34870  D loss: 0.07107  \n",
      "Train Epoch: [181/200]Step: [500/600] G loss: 7.27593  D loss: 0.10373  \n",
      "Train Epoch: [182/200]Step: [0/600]   G loss: 5.78356  D loss: 0.03662  \n",
      "Train Epoch: [182/200]Step: [100/600] G loss: 6.09361  D loss: 0.09815  \n",
      "Train Epoch: [182/200]Step: [200/600] G loss: 7.02180  D loss: 0.08241  \n",
      "Train Epoch: [182/200]Step: [300/600] G loss: 4.83511  D loss: 0.05630  \n",
      "Train Epoch: [182/200]Step: [400/600] G loss: 6.20373  D loss: 0.05211  \n",
      "Train Epoch: [182/200]Step: [500/600] G loss: 5.68977  D loss: 0.01320  \n",
      "Train Epoch: [183/200]Step: [0/600]   G loss: 6.46584  D loss: 0.01515  \n",
      "Train Epoch: [183/200]Step: [100/600] G loss: 6.00564  D loss: 0.10255  \n",
      "Train Epoch: [183/200]Step: [200/600] G loss: 6.38468  D loss: 0.02179  \n",
      "Train Epoch: [183/200]Step: [300/600] G loss: 6.02148  D loss: 0.03773  \n",
      "Train Epoch: [183/200]Step: [400/600] G loss: 6.91392  D loss: 0.05728  \n",
      "Train Epoch: [183/200]Step: [500/600] G loss: 5.77887  D loss: 0.04073  \n",
      "Train Epoch: [184/200]Step: [0/600]   G loss: 6.53857  D loss: 0.05670  \n",
      "Train Epoch: [184/200]Step: [100/600] G loss: 6.92768  D loss: 0.07521  \n",
      "Train Epoch: [184/200]Step: [200/600] G loss: 6.26477  D loss: 0.11425  \n",
      "Train Epoch: [184/200]Step: [300/600] G loss: 5.64315  D loss: 0.03518  \n",
      "Train Epoch: [184/200]Step: [400/600] G loss: 6.33067  D loss: 0.00525  \n",
      "Train Epoch: [184/200]Step: [500/600] G loss: 6.81167  D loss: 0.08460  \n",
      "Train Epoch: [185/200]Step: [0/600]   G loss: 6.93455  D loss: 0.02623  \n",
      "Train Epoch: [185/200]Step: [100/600] G loss: 7.27551  D loss: 0.02333  \n",
      "Train Epoch: [185/200]Step: [200/600] G loss: 6.33874  D loss: 0.08137  \n",
      "Train Epoch: [185/200]Step: [300/600] G loss: 7.21965  D loss: 0.06124  \n",
      "Train Epoch: [185/200]Step: [400/600] G loss: 7.44623  D loss: 0.15480  \n",
      "Train Epoch: [185/200]Step: [500/600] G loss: 6.21510  D loss: 0.12103  \n",
      "Train Epoch: [186/200]Step: [0/600]   G loss: 7.44517  D loss: 0.07303  \n",
      "Train Epoch: [186/200]Step: [100/600] G loss: 6.36360  D loss: 0.01960  \n",
      "Train Epoch: [186/200]Step: [200/600] G loss: 5.39546  D loss: 0.02876  \n",
      "Train Epoch: [186/200]Step: [300/600] G loss: 7.23447  D loss: 0.12747  \n",
      "Train Epoch: [186/200]Step: [400/600] G loss: 6.27339  D loss: 0.08470  \n",
      "Train Epoch: [186/200]Step: [500/600] G loss: 8.27273  D loss: 0.09405  \n",
      "Train Epoch: [187/200]Step: [0/600]   G loss: 6.54400  D loss: 0.02670  \n",
      "Train Epoch: [187/200]Step: [100/600] G loss: 7.17615  D loss: 0.08490  \n",
      "Train Epoch: [187/200]Step: [200/600] G loss: 6.19697  D loss: 0.05768  \n",
      "Train Epoch: [187/200]Step: [300/600] G loss: 5.81137  D loss: 0.08088  \n",
      "Train Epoch: [187/200]Step: [400/600] G loss: 6.10481  D loss: 0.09399  \n",
      "Train Epoch: [187/200]Step: [500/600] G loss: 5.03964  D loss: 0.09160  \n",
      "Train Epoch: [188/200]Step: [0/600]   G loss: 6.87171  D loss: 0.01463  \n",
      "Train Epoch: [188/200]Step: [100/600] G loss: 5.86763  D loss: 0.00940  \n",
      "Train Epoch: [188/200]Step: [200/600] G loss: 6.64953  D loss: 0.03339  \n",
      "Train Epoch: [188/200]Step: [300/600] G loss: 5.50132  D loss: 0.02822  \n",
      "Train Epoch: [188/200]Step: [400/600] G loss: 4.97169  D loss: 0.05220  \n",
      "Train Epoch: [188/200]Step: [500/600] G loss: 5.29280  D loss: 0.03065  \n",
      "Train Epoch: [189/200]Step: [0/600]   G loss: 6.82495  D loss: 0.09880  \n",
      "Train Epoch: [189/200]Step: [100/600] G loss: 6.68447  D loss: 0.10676  \n",
      "Train Epoch: [189/200]Step: [200/600] G loss: 8.12334  D loss: 0.14535  \n",
      "Train Epoch: [189/200]Step: [300/600] G loss: 5.82457  D loss: 0.02832  \n",
      "Train Epoch: [189/200]Step: [400/600] G loss: 5.86099  D loss: 0.02715  \n",
      "Train Epoch: [189/200]Step: [500/600] G loss: 6.14669  D loss: 0.04012  \n",
      "Train Epoch: [190/200]Step: [0/600]   G loss: 5.57224  D loss: 0.21011  \n",
      "Train Epoch: [190/200]Step: [100/600] G loss: 5.41703  D loss: 0.07583  \n",
      "Train Epoch: [190/200]Step: [200/600] G loss: 5.89921  D loss: 0.04252  \n",
      "Train Epoch: [190/200]Step: [300/600] G loss: 5.35019  D loss: 0.02531  \n",
      "Train Epoch: [190/200]Step: [400/600] G loss: 5.24354  D loss: 0.10710  \n",
      "Train Epoch: [190/200]Step: [500/600] G loss: 5.81882  D loss: 0.11756  \n",
      "Train Epoch: [191/200]Step: [0/600]   G loss: 5.23413  D loss: 0.11426  \n",
      "Train Epoch: [191/200]Step: [100/600] G loss: 4.22642  D loss: 0.05007  \n",
      "Train Epoch: [191/200]Step: [200/600] G loss: 5.33338  D loss: 0.04837  \n",
      "Train Epoch: [191/200]Step: [300/600] G loss: 5.12101  D loss: 0.05226  \n",
      "Train Epoch: [191/200]Step: [400/600] G loss: 7.16345  D loss: 0.10152  \n",
      "Train Epoch: [191/200]Step: [500/600] G loss: 6.58811  D loss: 0.03976  \n",
      "Train Epoch: [192/200]Step: [0/600]   G loss: 5.68130  D loss: 0.01086  \n",
      "Train Epoch: [192/200]Step: [100/600] G loss: 7.27536  D loss: 0.00455  \n",
      "Train Epoch: [192/200]Step: [200/600] G loss: 6.97799  D loss: 0.15070  \n",
      "Train Epoch: [192/200]Step: [300/600] G loss: 8.32586  D loss: 0.00856  \n",
      "Train Epoch: [192/200]Step: [400/600] G loss: 6.93399  D loss: 0.06306  \n",
      "Train Epoch: [192/200]Step: [500/600] G loss: 4.96527  D loss: 0.11930  \n",
      "Train Epoch: [193/200]Step: [0/600]   G loss: 7.05195  D loss: 0.01642  \n",
      "Train Epoch: [193/200]Step: [100/600] G loss: 7.31634  D loss: 0.02209  \n",
      "Train Epoch: [193/200]Step: [200/600] G loss: 7.31768  D loss: 0.00983  \n",
      "Train Epoch: [193/200]Step: [300/600] G loss: 5.88843  D loss: 0.01470  \n",
      "Train Epoch: [193/200]Step: [400/600] G loss: 7.54574  D loss: 0.05336  \n",
      "Train Epoch: [193/200]Step: [500/600] G loss: 6.15815  D loss: 0.00948  \n",
      "Train Epoch: [194/200]Step: [0/600]   G loss: 5.17900  D loss: 0.12257  \n",
      "Train Epoch: [194/200]Step: [100/600] G loss: 6.20747  D loss: 0.10661  \n",
      "Train Epoch: [194/200]Step: [200/600] G loss: 5.37291  D loss: 0.02790  \n",
      "Train Epoch: [194/200]Step: [300/600] G loss: 7.14645  D loss: 0.17879  \n",
      "Train Epoch: [194/200]Step: [400/600] G loss: 5.83744  D loss: 0.01652  \n",
      "Train Epoch: [194/200]Step: [500/600] G loss: 4.74309  D loss: 0.09413  \n",
      "Train Epoch: [195/200]Step: [0/600]   G loss: 6.30393  D loss: 0.17434  \n",
      "Train Epoch: [195/200]Step: [100/600] G loss: 6.69117  D loss: 0.18461  \n",
      "Train Epoch: [195/200]Step: [200/600] G loss: 6.59894  D loss: 0.02775  \n",
      "Train Epoch: [195/200]Step: [300/600] G loss: 5.57364  D loss: 0.04891  \n",
      "Train Epoch: [195/200]Step: [400/600] G loss: 6.82674  D loss: 0.16791  \n",
      "Train Epoch: [195/200]Step: [500/600] G loss: 8.10294  D loss: 0.01637  \n",
      "Train Epoch: [196/200]Step: [0/600]   G loss: 6.69307  D loss: 0.10198  \n",
      "Train Epoch: [196/200]Step: [100/600] G loss: 7.73993  D loss: 0.13770  \n",
      "Train Epoch: [196/200]Step: [200/600] G loss: 7.88565  D loss: 0.13004  \n",
      "Train Epoch: [196/200]Step: [300/600] G loss: 6.79121  D loss: 0.03645  \n",
      "Train Epoch: [196/200]Step: [400/600] G loss: 6.51787  D loss: 0.00688  \n",
      "Train Epoch: [196/200]Step: [500/600] G loss: 6.46778  D loss: 0.00923  \n",
      "Train Epoch: [197/200]Step: [0/600]   G loss: 7.49051  D loss: 0.03384  \n",
      "Train Epoch: [197/200]Step: [100/600] G loss: 6.07321  D loss: 0.02029  \n",
      "Train Epoch: [197/200]Step: [200/600] G loss: 5.68632  D loss: 0.08442  \n",
      "Train Epoch: [197/200]Step: [300/600] G loss: 6.77992  D loss: 0.00604  \n",
      "Train Epoch: [197/200]Step: [400/600] G loss: 7.04986  D loss: 0.04829  \n",
      "Train Epoch: [197/200]Step: [500/600] G loss: 5.40701  D loss: 0.05272  \n",
      "Train Epoch: [198/200]Step: [0/600]   G loss: 5.61130  D loss: 0.02047  \n",
      "Train Epoch: [198/200]Step: [100/600] G loss: 6.07213  D loss: 0.02000  \n",
      "Train Epoch: [198/200]Step: [200/600] G loss: 6.41135  D loss: 0.00518  \n",
      "Train Epoch: [198/200]Step: [300/600] G loss: 5.65610  D loss: 0.00908  \n",
      "Train Epoch: [198/200]Step: [400/600] G loss: 6.45341  D loss: 0.05392  \n",
      "Train Epoch: [198/200]Step: [500/600] G loss: 5.66257  D loss: 0.11440  \n",
      "Train Epoch: [199/200]Step: [0/600]   G loss: 5.67066  D loss: 0.14141  \n",
      "Train Epoch: [199/200]Step: [100/600] G loss: 7.29911  D loss: 0.00625  \n",
      "Train Epoch: [199/200]Step: [200/600] G loss: 8.13512  D loss: 0.06968  \n",
      "Train Epoch: [199/200]Step: [300/600] G loss: 6.76131  D loss: 0.01592  \n",
      "Train Epoch: [199/200]Step: [400/600] G loss: 5.92202  D loss: 0.02260  \n",
      "Train Epoch: [199/200]Step: [500/600] G loss: 7.19016  D loss: 0.08796  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9ccee3d186a5cde1a10dc2e669ca70ad17010fdccdd2a33eff749a1f85506937"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}